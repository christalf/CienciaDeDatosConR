<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Exploratory Data Analysis Course - Module 3</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Aprendiendo ciencia de datos con R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="about.html">
    <span class="fa fa-hand-spock"></span>
     
    Resumen del contenido
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Contenido
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="divider"></li>
    <li class="dropdown-header">Data Science Toolbox</li>
    <li class="divider"></li>
    <li>
      <a href="1_DataScienceToolBox_Resumen.html">Módulo único</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">R</li>
    <li class="divider"></li>
    <li>
      <a href="2_R_Module1_Resumen.html">Módulo 1</a>
    </li>
    <li>
      <a href="2_R_Module2_Resumen.html">Módulo 2</a>
    </li>
    <li>
      <a href="2_R_Module2_ProgAssmt_PrevPract.html">_práctica</a>
    </li>
    <li>
      <a href="2_R_Module2_ProgAssmt.html">_ejercicio</a>
    </li>
    <li>
      <a href="2_R_Module3_Resumen.html">Módulo 3</a>
    </li>
    <li>
      <a href="2_R_Module3_ProgAssmt.html">_ejercicio</a>
    </li>
    <li>
      <a href="2_R_Module4_Resumen.html">Módulo 4</a>
    </li>
    <li>
      <a href="2_R_Module4_ProgAssmt.html">Ejercicio final</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Getting &amp; Cleaning Data</li>
    <li class="divider"></li>
    <li>
      <a href="3_GettingAndCleaningData_Module1_Resumen.html">Módulo 1</a>
    </li>
    <li>
      <a href="3_GettingAndCleaningData_Module2_Resumen.html">Módulo 2</a>
    </li>
    <li>
      <a href="3_GettingAndCleaningData_Module2_Ejercicios.html">_ejercicios</a>
    </li>
    <li>
      <a href="3_GettingAndCleaningData_Module3_Resumen.html">Módulo 3</a>
    </li>
    <li>
      <a href="3_GettingAndCleaningData_Module3_Ejercicios.html">_ejercicios</a>
    </li>
    <li>
      <a href="3_GettingAndCleaningData_Module4.html">Módulo 4</a>
    </li>
    <li>
      <a href="3_GettingAndCleaningData_Module4_Ejercicios.html">_ejercicios</a>
    </li>
    <li>
      <a href="3_GettingAndCleaningData_FinalProj.html">Proyecto final</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Exploratory Data Analysis</li>
    <li class="divider"></li>
    <li>
      <a href="4_ExploratoryDataAnalysis_Module1.html">Módulo 1</a>
    </li>
    <li>
      <a href="4_ExploratoryDataAnalysis_Module1_Ejercicios.html">_ejercicios</a>
    </li>
    <li>
      <a href="4_ExploratoryDataAnalysis_Module2.html">Módulo 2</a>
    </li>
    <li>
      <a href="4_ExploratoryDataAnalysis_Module3.html">Módulo 3</a>
    </li>
    <li>
      <a href="4_ExploratoryDataAnalysis_Module4.html">Módulo 4</a>
    </li>
    <li>
      <a href="4_ExploratoryDataAnalysis_FinalProj.html">Proyecto final</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Reproducible Research</li>
    <li class="divider"></li>
    <li>
      <a href="5_ReproducibleResearch_Module1.html">Módulo 1</a>
    </li>
    <li>
      <a href="5_ReproducibleResearch_Module2.html">Módulo 2</a>
    </li>
    <li>
      <a href="5_ReproducibleResearch_Module3.html">Módulo 3</a>
    </li>
    <li>
      <a href="5_ReproducibleResearch_Module4.html">Módulo 4</a>
    </li>
    <li>
      <a href="5_ReproducibleResearch_FinalProj.html">Proyectos finales</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://bookdown.org/yihui/rmarkdown/rmarkdown-site.html">
    <span class="fa fa-gears"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Exploratory Data Analysis Course - Module
3</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#hierarchical-clustering."
id="toc-hierarchical-clustering."><span
class="toc-section-number">1</span> Hierarchical Clustering.</a>
<ul>
<li><a href="#the-three-aspects-of-hierarchical-clustering."
id="toc-the-three-aspects-of-hierarchical-clustering."><span
class="toc-section-number">1.1</span> The three aspects of hierarchical
clustering.</a>
<ul>
<li><a href="#what-is-it-an-aglomerative-approach."
id="toc-what-is-it-an-aglomerative-approach."><span
class="toc-section-number">1.1.1</span> What is it: an aglomerative
approach.</a></li>
<li><a
href="#what-does-it-require-a-closeness-definition-and-a-merging-approach."
id="toc-what-does-it-require-a-closeness-definition-and-a-merging-approach."><span
class="toc-section-number">1.1.2</span> What does it require: a
closeness definition and a merging approach.</a></li>
<li><a href="#what-does-it-produce-a-tree-dendrogram."
id="toc-what-does-it-produce-a-tree-dendrogram."><span
class="toc-section-number">1.1.3</span> What does it produce: a tree
(dendrogram).</a></li>
</ul></li>
<li><a href="#example-distances." id="toc-example-distances."><span
class="toc-section-number">1.2</span> Example distances.</a>
<ul>
<li><a href="#euclidean-distance." id="toc-euclidean-distance."><span
class="toc-section-number">1.2.1</span> Euclidean distance.</a></li>
<li><a href="#manhattan-distance." id="toc-manhattan-distance."><span
class="toc-section-number">1.2.2</span> Manhattan distance.</a></li>
</ul></li>
<li><a href="#hierarchical-clustering-example."
id="toc-hierarchical-clustering-example."><span
class="toc-section-number">1.3</span> Hierarchical clustering
example.</a>
<ul>
<li><a href="#first-step-calculating-the-distance-matrix-with-dist."
id="toc-first-step-calculating-the-distance-matrix-with-dist."><span
class="toc-section-number">1.3.1</span> First step: calculating the
distance matrix with <code>dist()</code>.</a></li>
<li><a href="#finding-the-two-closest-points."
id="toc-finding-the-two-closest-points."><span
class="toc-section-number">1.3.2</span> Finding the two closest
points.</a></li>
<li><a
href="#merging-those-two-closest-points-and-begin-building-the-tree-hclust-and-as.dendrogram."
id="toc-merging-those-two-closest-points-and-begin-building-the-tree-hclust-and-as.dendrogram."><span
class="toc-section-number">1.3.3</span> Merging those two closest points
and begin building the tree: <code>hclust()</code> and
<code>as.dendrogram()</code>.</a></li>
<li><a
href="#restarting-the-algorithm-the-next-pair-of-closest-points-and-its-merged-tree-leaves."
id="toc-restarting-the-algorithm-the-next-pair-of-closest-points-and-its-merged-tree-leaves."><span
class="toc-section-number">1.3.4</span> Restarting the algorithm: the
next pair of closest points and its merged tree leaves.</a></li>
<li><a href="#stopping-the-algorithm."
id="toc-stopping-the-algorithm."><span
class="toc-section-number">1.3.5</span> Stopping the algorithm.</a></li>
</ul></li>
<li><a href="#the-entire-tree-dendrogram-at-once-with-hclust."
id="toc-the-entire-tree-dendrogram-at-once-with-hclust."><span
class="toc-section-number">1.4</span> The entire tree (dendrogram) at
once, with <code>hclust()</code>.</a>
<ul>
<li><a href="#how-to-determine-how-many-clusters-are-there"
id="toc-how-to-determine-how-many-clusters-are-there"><span
class="toc-section-number">1.4.1</span> How to determine how many
clusters are there?</a></li>
<li><a
href="#adjusting-a-bit-the-plotting-of-the-dendrogram-and-cutting-it-with-horizontal-lines."
id="toc-adjusting-a-bit-the-plotting-of-the-dendrogram-and-cutting-it-with-horizontal-lines."><span
class="toc-section-number">1.4.2</span> Adjusting a bit the plotting of
the dendrogram and cutting it with horizontal lines.</a></li>
<li><a
href="#the-heights-of-horizontal-lines-in-a-dendrogram-are-distances."
id="toc-the-heights-of-horizontal-lines-in-a-dendrogram-are-distances."><span
class="toc-section-number">1.4.3</span> The heights of horizontal lines
in a dendrogram are distances.</a></li>
</ul></li>
<li><a href="#prettier-dendrograms."
id="toc-prettier-dendrograms."><span
class="toc-section-number">1.5</span> Prettier dendrograms.</a>
<ul>
<li><a
href="#a-custom-function-that-add-colors-to-the-output-of-hclust."
id="toc-a-custom-function-that-add-colors-to-the-output-of-hclust."><span
class="toc-section-number">1.5.1</span> A custom function that add
colors to the output of <code>hclust()</code>.</a></li>
<li><a href="#a-lot-of-prettier-dendrograms-sources."
id="toc-a-lot-of-prettier-dendrograms-sources."><span
class="toc-section-number">1.5.2</span> A lot of prettier dendrograms
sources.</a></li>
</ul></li>
<li><a href="#the-agglomeration-methods."
id="toc-the-agglomeration-methods."><span
class="toc-section-number">1.6</span> The agglomeration methods.</a>
<ul>
<li><a href="#complete-linkage." id="toc-complete-linkage."><span
class="toc-section-number">1.6.1</span> Complete linkage.</a></li>
<li><a href="#average-linkage." id="toc-average-linkage."><span
class="toc-section-number">1.6.2</span> Average linkage.</a></li>
<li><a href="#which-method-to-choose"
id="toc-which-method-to-choose"><span
class="toc-section-number">1.6.3</span> Which method to choose?</a></li>
</ul></li>
<li><a href="#the-heatmap-function."
id="toc-the-heatmap-function."><span
class="toc-section-number">1.7</span> The <code>heatmap()</code>
function.</a>
<ul>
<li><a href="#a-short-tutorial-for-decent-heat-maps-in-r."
id="toc-a-short-tutorial-for-decent-heat-maps-in-r."><span
class="toc-section-number">1.7.1</span> A short tutorial for decent heat
maps in R.</a></li>
</ul></li>
<li><a href="#final-notes-and-further-resources."
id="toc-final-notes-and-further-resources."><span
class="toc-section-number">1.8</span> Final notes and further
resources.</a></li>
</ul></li>
<li><a href="#k-means-culstering." id="toc-k-means-culstering."><span
class="toc-section-number">2</span> K-Means Culstering.</a>
<ul>
<li><a href="#the-three-aspects-of-k-means-clustering."
id="toc-the-three-aspects-of-k-means-clustering."><span
class="toc-section-number">2.1</span> The three aspects of k-means
clustering.</a>
<ul>
<li><a href="#what-is-it-a-partitioning-approach."
id="toc-what-is-it-a-partitioning-approach."><span
class="toc-section-number">2.1.1</span> What is it: a partitioning
approach.</a></li>
<li><a
href="#what-does-it-require-a-distance-metric-a-fixed-number-of-clusters-and-an-initial-centroids-guess."
id="toc-what-does-it-require-a-distance-metric-a-fixed-number-of-clusters-and-an-initial-centroids-guess."><span
class="toc-section-number">2.1.2</span> What does it require: a distance
metric, a fixed number of clusters and an initial centroids
guess.</a></li>
<li><a
href="#what-does-it-produce-final-estimate-of-cluster-centroids-and-points-assignment"
id="toc-what-does-it-produce-final-estimate-of-cluster-centroids-and-points-assignment"><span
class="toc-section-number">2.1.3</span> What does it produce: final
estimate of cluster centroids and points assignment</a></li>
</ul></li>
<li><a href="#k-means-clustering-example."
id="toc-k-means-clustering-example."><span
class="toc-section-number">2.2</span> K-means clustering example.</a>
<ul>
<li><a href="#first-step-starting-centroids."
id="toc-first-step-starting-centroids."><span
class="toc-section-number">2.2.1</span> First step: starting
centroids.</a></li>
<li><a href="#assigning-points-to-closest-centroid."
id="toc-assigning-points-to-closest-centroid."><span
class="toc-section-number">2.2.2</span> Assigning points to closest
centroid.</a></li>
<li><a href="#recalculating-centroids."
id="toc-recalculating-centroids."><span
class="toc-section-number">2.2.3</span> Recalculating
centroids.</a></li>
<li><a href="#reassigning-values-and-updating-centroids."
id="toc-reassigning-values-and-updating-centroids."><span
class="toc-section-number">2.2.4</span> Reassigning values and updating
centroids.</a></li>
<li><a href="#stopping-the-algorithm.-1"
id="toc-stopping-the-algorithm.-1"><span
class="toc-section-number">2.2.5</span> Stopping the algorithm.</a></li>
<li><a href="#the-algorithm-at-once-with-kmeans-function."
id="toc-the-algorithm-at-once-with-kmeans-function."><span
class="toc-section-number">2.2.6</span> The algorithm at once with
<code>kmeans()</code> function.</a></li>
</ul></li>
<li><a href="#building-heatmaps-from-k-means-solutions."
id="toc-building-heatmaps-from-k-means-solutions."><span
class="toc-section-number">2.3</span> Building heatmaps from K-means
solutions.</a></li>
<li><a href="#notes-and-further-resources."
id="toc-notes-and-further-resources."><span
class="toc-section-number">2.4</span> Notes and further
resources.</a></li>
</ul></li>
<li><a
href="#dimension-reduction-singular-value-decomposition--svd--and-principal-components-analysis--pca-."
id="toc-dimension-reduction-singular-value-decomposition--svd--and-principal-components-analysis--pca-."><span
class="toc-section-number">3</span> Dimension Reduction: Singular Value
Decomposition -SVD- and Principal Components Analysis -PCA-.</a>
<ul>
<li><a
href="#svd-and-pca-as-statistical-techniques-for-high-dimensional-matrix-data."
id="toc-svd-and-pca-as-statistical-techniques-for-high-dimensional-matrix-data."><span
class="toc-section-number">3.1</span> SVD and PCA as statistical
techniques for high-dimensional matrix data.</a></li>
<li><a href="#simulating-purely-random-matrix-data."
id="toc-simulating-purely-random-matrix-data."><span
class="toc-section-number">3.2</span> Simulating purely random matrix
data.</a></li>
<li><a
href="#applying-hierarchical-clustering-to-explore-the-matrix-data."
id="toc-applying-hierarchical-clustering-to-explore-the-matrix-data."><span
class="toc-section-number">3.3</span> Applying hierarchical clustering
to explore the matrix data.</a></li>
<li><a href="#adding-a-pattern-to-our-matrix-data-to-see-what-happens."
id="toc-adding-a-pattern-to-our-matrix-data-to-see-what-happens."><span
class="toc-section-number">3.4</span> Adding a pattern to our matrix
data to see what happens.</a></li>
<li><a
href="#applying-hierarchical-clustering-to-explore-the-matrix-data.-1"
id="toc-applying-hierarchical-clustering-to-explore-the-matrix-data.-1"><span
class="toc-section-number">3.5</span> Applying hierarchical clustering
to explore the matrix data.</a></li>
<li><a href="#general-case-matrices-with-patterns-in-rows-and-columns."
id="toc-general-case-matrices-with-patterns-in-rows-and-columns."><span
class="toc-section-number">3.6</span> General case: matrices with
patterns in rows and columns.</a></li>
<li><a href="#more-complex-situations."
id="toc-more-complex-situations."><span
class="toc-section-number">3.7</span> More complex situations.</a></li>
<li><a href="#related-problems." id="toc-related-problems."><span
class="toc-section-number">3.8</span> Related problems.</a>
<ul>
<li><a href="#one-kind-of-problem--a-pca-kind-of-problem-"
id="toc-one-kind-of-problem--a-pca-kind-of-problem-"><span
class="toc-section-number">3.8.1</span> One kind of problem -a PCA kind
of problem-</a></li>
<li><a href="#another-kind-of-problem--a-svd-kind-of-problem-"
id="toc-another-kind-of-problem--a-svd-kind-of-problem-"><span
class="toc-section-number">3.8.2</span> Another kind of problem -a SVD
kind of problem-</a></li>
</ul></li>
<li><a href="#related-solutions-svd-and-pca."
id="toc-related-solutions-svd-and-pca."><span
class="toc-section-number">3.9</span> Related solutions: SVD and
PCA.</a>
<ul>
<li><a href="#svd-matrix-decomposition."
id="toc-svd-matrix-decomposition."><span
class="toc-section-number">3.9.1</span> SVD matrix
decomposition.</a></li>
<li><a href="#pca-as-an-application-of-svd."
id="toc-pca-as-an-application-of-svd."><span
class="toc-section-number">3.9.2</span> PCA as an application of
SVD.</a></li>
<li><a href="#unpacking-the-svd-the-svd-function."
id="toc-unpacking-the-svd-the-svd-function."><span
class="toc-section-number">3.9.3</span> Unpacking the SVD: the
<code>svd()</code> function.</a></li>
<li><a href="#svd-for-data-compression."
id="toc-svd-for-data-compression."><span
class="toc-section-number">3.9.4</span> SVD for data
compression.</a></li>
</ul></li>
<li><a
href="#statistical-interpretation-of-components-of-the-svd-variance-explained-by-each-singular-value."
id="toc-statistical-interpretation-of-components-of-the-svd-variance-explained-by-each-singular-value."><span
class="toc-section-number">3.10</span> Statistical interpretation of
components of the SVD: variance explained by each singular
value.</a></li>
<li><a
href="#the-related-principal-components-analysis--pca--prcomp-function."
id="toc-the-related-principal-components-analysis--pca--prcomp-function."><span
class="toc-section-number">3.11</span> The related principal components
analysis -PCA-: <code>prcomp()</code> function.</a></li>
<li><a href="#what-if-we-add-a-second-pattern"
id="toc-what-if-we-add-a-second-pattern"><span
class="toc-section-number">3.12</span> What if we add a second
pattern?</a>
<ul>
<li><a href="#plotting-the-true-patterns-which-normally-we-ignore."
id="toc-plotting-the-true-patterns-which-normally-we-ignore."><span
class="toc-section-number">3.12.1</span> Plotting the <em>true</em>
patterns which normally we ignore.</a></li>
<li><a
href="#can-svdpca-analysis-detect-these-patterns-just-from-the-data"
id="toc-can-svdpca-analysis-detect-these-patterns-just-from-the-data"><span
class="toc-section-number">3.12.2</span> Can SVD/PCA analysis detect
these patterns just from the data?</a>
<ul>
<li><a href="#v-to-pick-up-column-patterns-of-variance."
id="toc-v-to-pick-up-column-patterns-of-variance."><span
class="toc-section-number">3.12.2.1</span> <span
class="math inline">\(V\)</span> to pick up column patterns of
variance.</a></li>
<li><a href="#d-and-variance-explained."
id="toc-d-and-variance-explained."><span
class="toc-section-number">3.12.2.2</span> <span
class="math inline">\(D\)</span> and variance explained.</a></li>
</ul></li>
</ul></li>
<li><a href="#dealing-with-missing-values."
id="toc-dealing-with-missing-values."><span
class="toc-section-number">3.13</span> Dealing with missing values.</a>
<ul>
<li><a href="#the-impute-package-from-bioconductor."
id="toc-the-impute-package-from-bioconductor."><span
class="toc-section-number">3.13.1</span> The <code>impute</code> package
from Bioconductor.</a></li>
</ul></li>
<li><a href="#face-example-producing-approximations-with-svd."
id="toc-face-example-producing-approximations-with-svd."><span
class="toc-section-number">3.14</span> Face example: producing
approximations with SVD.</a>
<ul>
<li><a href="#variance-explained." id="toc-variance-explained."><span
class="toc-section-number">3.14.1</span> Variance explained.</a></li>
<li><a href="#create-approximations."
id="toc-create-approximations."><span
class="toc-section-number">3.14.2</span> Create approximations.</a></li>
<li><a href="#plot-approximations." id="toc-plot-approximations."><span
class="toc-section-number">3.14.3</span> Plot approximations.</a></li>
</ul></li>
<li><a href="#notes-and-further-resources"
id="toc-notes-and-further-resources"><span
class="toc-section-number">3.15</span> Notes and further
resources</a></li>
</ul></li>
<li><a href="#session-info." id="toc-session-info."><span
class="toc-section-number">4</span> Session Info.</a></li>
</ul>
</div>

<hr />
<div id="hierarchical-clustering." class="section level1" number="1">
<h1><span class="header-section-number">1</span> Hierarchical
Clustering.</h1>
<p><strong>Clustering</strong> or <strong>cluster analysis</strong> is a
technique for <em>visualizing high</em> <em>dimensional or
multidimensional</em> data.</p>
<ul>
<li>A really quick way to get a sense of what’s going on in a very high
dimensional dataset.<br />
</li>
<li>A widely used technique, applied in many different areas of science,
business and other applications.</li>
</ul>
<div id="the-three-aspects-of-hierarchical-clustering."
class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> The three aspects of
hierarchical clustering.</h2>
<div id="what-is-it-an-aglomerative-approach." class="section level3"
number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> What is it: an
aglomerative approach.</h3>
<p>Hierarchical clustering, as is denoted by the name, <strong>involves
organizing</strong> our data <strong>into a kind of
hierarchy</strong>.</p>
<p>The common approach is what’s called <strong>an agglomerative
approach</strong>.</p>
<ul>
<li><p>This is a kind of <em>bottom up approach</em>, where we start by
thinking of the data as individual data points. Then we start lumping
them together into clusters little by little until eventually our entire
dataset is just one big cluster.</p></li>
<li><p>The <strong>algorithm is recursive</strong> and goes as
follows:</p>
<ol style="list-style-type: decimal">
<li><strong><em>Find closest two things/points</em></strong> in your
dataset.<br />
</li>
<li><strong><em>Put them together</em></strong> and call them a
“point”.<br />
</li>
<li>Use your <strong>new “dataset”</strong> with this new point and
<strong>repeat</strong>.</li>
</ol></li>
</ul>
<blockquote>
<p>In short, Clustering <strong>organizes</strong> things
(data/observations) that are <strong>close</strong> into
<strong>groups</strong>.</p>
</blockquote>
</div>
<div
id="what-does-it-require-a-closeness-definition-and-a-merging-approach."
class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> What does it
require: a closeness definition and a merging approach.</h3>
<p>This methodology requires us to answer two questions:</p>
<ol style="list-style-type: decimal">
<li><strong>How do we define</strong> <strong><em>close?</em></strong>
<ul>
<li>We have to pick-up a <strong>way to measure the distance</strong>
between two points, <em>that make sense</em> for our problem.<br />
</li>
<li>The commonly used <strong><em>distance metrics</em></strong> (or
inversely <em>similarity metrics</em>) are:
<ul>
<li><strong>Euclidean</strong> distance:
<ul>
<li>A <em>continuous metric</em> which can be thought of in geometric
terms as the <em>straight-line distance</em> between two points.<br />
</li>
</ul></li>
<li><strong>Correlation</strong> similarity:
<ul>
<li>Also a <em>continuous metric</em> similar to Euclidean
distance.<br />
</li>
</ul></li>
<li><strong>Manhattan</strong> distance:
<ul>
<li>A <em>binary metric</em> that consist of the number of “city blocks”
on a grid or lattice, between two points.<br />
</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>How do we group things?</strong>
<ul>
<li>In other words, have an <strong>approach to merging</strong> two
points to create a new “point”.</li>
</ul></li>
</ol>
</div>
<div id="what-does-it-produce-a-tree-dendrogram." class="section level3"
number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> What does it
produce: a tree (dendrogram).</h3>
<p>Other two questions that we are going to answer, are:</p>
<ul>
<li>How do we <strong>visualize</strong> the <em>grouping</em> in
hierarchical clustering?<br />
</li>
<li>How do we <strong>interpret</strong> the <em>grouping?</em></li>
</ul>
<p>A benefit of the clustering methodology is that we can
<strong>produce a tree</strong> <em>showing how close</em> things are to
each other, which is simply a product of running the algorithm.</p>
</div>
</div>
<div id="example-distances." class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Example
distances.</h2>
<div id="euclidean-distance." class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Euclidean
distance.</h3>
<p><img src="4_data/euclidean1.png" /></p>
<p>For example, take two cities, say, Baltimore and Washington D.C., and
put them on a map.</p>
<p>If we imagine that the center of each city has an X and a Y
coordinate (<em>longitude</em> and <em>latitude</em>) and we want to map
the <strong>distance between the centers</strong> of the two cities,
then we can draw a <strong>straight diagonal line</strong> between the
two cities.</p>
<p>The distance can be calculated in the usual way:</p>
<p><span class="math display">\[\sqrt{(X_1-X_2)^2 +
(Y_1-Y_2)^2}\]</span></p>
<p>One nice feature of Euclidean distance is that it’s <strong>easily
generalizable</strong> to higher dimensions.</p>
<p>In general, given two points in an <em>n-dimensional</em> space <span
class="math display">\[A = (A_1, A_2, \ldots, A_n)\]</span> and <span
class="math display">\[B = (B_1, B_2, \ldots, B_n)\]</span> the
Eculidean distance between them is: <span
class="math display">\[\sqrt{(A_1-B_1)^2 + (A_2-B_2)^2 + \ldots +
(A_n-B_n)^2}\]</span></p>
</div>
<div id="manhattan-distance." class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Manhattan
distance.</h3>
<p>The Manhattan distance gets its name from the idea that you can look
at <strong>points as</strong> <strong>being on a grid or
lattice</strong>, like the grid making up the streets of
<em>Manhattan</em> in New York City.</p>
<p><img src="4_data/manhattan.png" /></p>
<ul>
<li>The red, blue and yellow lines show various way of getting between
the two black circles using the grid layout.<br />
</li>
<li>While the green line shows the Euclidean distance.</li>
</ul>
<p>The Manhattan distance between the points is <strong>simply the
sum</strong> of the <em>right-left</em> moves plus the sum of all the
<em>up-down</em> moves on the grid.</p>
<p>In general: <span class="math display">\[|A_1-B_1| + |A_2-B_2| +
\ldots + |A_n-B_n|\]</span></p>
</div>
</div>
<div id="hierarchical-clustering-example." class="section level2"
number="1.3">
<h2><span class="header-section-number">1.3</span> Hierarchical
clustering example.</h2>
<p>Let’s <strong>simulate</strong> some data in <strong>three separate
clusters</strong>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">12</span>, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> <span class="dv">4</span>), <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">12</span>, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">each =</span> <span class="dv">4</span>), <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x<span class="fl">+0.05</span>, y<span class="fl">+0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div id="first-step-calculating-the-distance-matrix-with-dist."
class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> First step:
calculating the distance matrix with <code>dist()</code>.</h3>
<p>The first step in the basic clustering approach is to
<strong>calculate the distance</strong> between <em>every point</em>
with <em>every other point</em>.</p>
<ul>
<li>The <strong>result</strong> is a <strong>distance
matrix</strong>.<br />
</li>
<li><strong><em>Notice:</em></strong>
<ul>
<li>Usually we will <em>not have to explicitly compute</em> the distance
matrix (unless we are inventing our own clustering method).<br />
</li>
<li>Here we’re just showing what’s going on internally.</li>
</ul></li>
</ul>
<p>The distance matrix can be computed with <strong>the
<code>dist()</code> function</strong> in R.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(dist)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span> (x, <span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>, <span class="at">diag =</span> <span class="cn">FALSE</span>, <span class="at">upper =</span> <span class="cn">FALSE</span>, <span class="at">p =</span> <span class="dv">2</span>)  </span></code></pre></div>
<ul>
<li><code>x</code> can be a numeric matix or data frame.<br />
</li>
<li><code>method</code> is the distance measure to be used, and must be
one of:
<ul>
<li>“euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or
“minkowski”.<br />
</li>
<li>The <strong>default distance metric</strong> is <em>Euclidean
distance</em>.<br />
</li>
</ul></li>
<li>This function computes the <strong>distances between the
rows</strong> of <code>x</code> and <strong>returns</strong> an object
of class <code>dist</code>.</li>
</ul>
<p>Turning back to our example, let’s calculate it’s associated distance
matrix.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>dataFrame <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dist</span>(dataFrame)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>            <span class="dv">1</span>          <span class="dv">2</span>          <span class="dv">3</span>          <span class="dv">4</span>          <span class="dv">5</span>          <span class="dv">6</span>          <span class="dv">7</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>  <span class="fl">0.34120511</span>                                                                  </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span>  <span class="fl">0.57493739</span> <span class="fl">0.24102750</span>                                                       </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>  <span class="fl">0.26381786</span> <span class="fl">0.52578819</span> <span class="fl">0.71861759</span>                                            </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>  <span class="fl">1.69424700</span> <span class="fl">1.35818182</span> <span class="fl">1.11952883</span> <span class="fl">1.80666768</span>                                 </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>  <span class="fl">1.65812902</span> <span class="fl">1.31960442</span> <span class="fl">1.08338841</span> <span class="fl">1.78081321</span> <span class="fl">0.08150268</span>                      </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>  <span class="fl">1.49823399</span> <span class="fl">1.16620981</span> <span class="fl">0.92568723</span> <span class="fl">1.60131659</span> <span class="fl">0.21110433</span> <span class="fl">0.21666557</span>           </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="dv">8</span>  <span class="fl">1.99149025</span> <span class="fl">1.69093111</span> <span class="fl">1.45648906</span> <span class="fl">2.02849490</span> <span class="fl">0.61704200</span> <span class="fl">0.69791931</span> <span class="fl">0.65062566</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span>  <span class="fl">2.13629539</span> <span class="fl">1.83167669</span> <span class="fl">1.67835968</span> <span class="fl">2.35675598</span> <span class="fl">1.18349654</span> <span class="fl">1.11500116</span> <span class="fl">1.28582631</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span> <span class="fl">2.06419586</span> <span class="fl">1.76999236</span> <span class="fl">1.63109790</span> <span class="fl">2.29239480</span> <span class="fl">1.23847877</span> <span class="fl">1.16550201</span> <span class="fl">1.32063059</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span> <span class="fl">2.14702468</span> <span class="fl">1.85183204</span> <span class="fl">1.71074417</span> <span class="fl">2.37461984</span> <span class="fl">1.28153948</span> <span class="fl">1.21077373</span> <span class="fl">1.37369662</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="dv">12</span> <span class="fl">2.05664233</span> <span class="fl">1.74662555</span> <span class="fl">1.58658782</span> <span class="fl">2.27232243</span> <span class="fl">1.07700974</span> <span class="fl">1.00777231</span> <span class="fl">1.17740375</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            <span class="dv">8</span>          <span class="dv">9</span>         <span class="dv">10</span>         <span class="dv">11</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>                                             </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span>                                             </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>                                             </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>                                             </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>                                             </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>                                             </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="dv">8</span>                                             </span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span>  <span class="fl">1.76460709</span>                                 </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span> <span class="fl">1.83517785</span> <span class="fl">0.14090406</span>                      </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span> <span class="fl">1.86999431</span> <span class="fl">0.11624471</span> <span class="fl">0.08317570</span>           </span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="dv">12</span> <span class="fl">1.66223814</span> <span class="fl">0.10848966</span> <span class="fl">0.19128645</span> <span class="fl">0.20802789</span></span></code></pre></div>
<ul>
<li>The output is a <strong>lower triangular matrix</strong> with rows
numbered from 2 to 12 and columns numbered from 1 to 11.<br />
</li>
<li>Entry (i,j) indicates the distance between points i and j.<br />
</li>
<li>Clearly we need only a lower triangular matrix since the
<em>distance between</em> points <strong>i and j</strong>
<em>equals</em> that between <strong>j and i</strong>.</li>
</ul>
</div>
<div id="finding-the-two-closest-points." class="section level3"
number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Finding the two
closest points.</h3>
<p>As we know, an agglomerative clustering approach attempts to find the
two points that are closest together.</p>
<ul>
<li>In other words, we want to <strong>find the smallest non-zero
entry</strong> in the <em>distance</em> <em>matrix</em>.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>rdistxy <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">dist</span>(dataFrame))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Remove the diagonal from consideration</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(rdistxy) <span class="ot">&lt;-</span> <span class="fu">diag</span>(rdistxy) <span class="sc">+</span> <span class="fl">1e5</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the index of the points with minimum distance</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(rdistxy <span class="sc">==</span> <span class="fu">min</span>(rdistxy), <span class="at">arr.ind=</span><span class="cn">TRUE</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>ind</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  row col</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>   <span class="dv">6</span>   <span class="dv">5</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>   <span class="dv">5</span>   <span class="dv">6</span></span></code></pre></div>
<p>Now we can plot the points and <strong>show which two points are
closest together</strong> according to our distance metric.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[ind[<span class="dv">1</span>, ]], y[ind[<span class="dv">1</span>, ]], <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div
id="merging-those-two-closest-points-and-begin-building-the-tree-hclust-and-as.dendrogram."
class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Merging those two
closest points and begin building the tree: <code>hclust()</code> and
<code>as.dendrogram()</code>.</h3>
<p>The next step for the algorithm is to start drawing the tree, the
first step of which would be to “merge” these two points together.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.3</span>))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">main =</span> <span class="st">&quot;Data&quot;</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[ind[<span class="dv">1</span>, ]], y[ind[<span class="dv">1</span>, ]], <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">mean</span>(x[ind[<span class="dv">1</span>, ]]), <span class="fu">mean</span>(y[ind[<span class="dv">1</span>, ]]), <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">cex =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">3</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">mean</span>(x[ind[<span class="dv">1</span>, ]]), <span class="fu">mean</span>(y[ind[<span class="dv">1</span>, ]]), <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">cex =</span> <span class="dv">5</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">1</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a cluster and cut it at the right height</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>hcluster <span class="ot">&lt;-</span> <span class="fu">dist</span>(dataFrame) <span class="sc">%&gt;%</span> hclust</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>dendro <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span>(hcluster)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>cutDendro <span class="ot">&lt;-</span> <span class="fu">cut</span>(dendro, <span class="at">h =</span> (hcluster<span class="sc">$</span>height[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.00001</span>))</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cutDendro<span class="sc">$</span>lower[[<span class="dv">11</span>]], <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Begin building tree&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
</div>
<div
id="restarting-the-algorithm-the-next-pair-of-closest-points-and-its-merged-tree-leaves."
class="section level3" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> Restarting the
algorithm: the next pair of closest points and its merged tree
leaves.</h3>
<p>Now that we’ve merged the first two “leaves” of this tree, we can
re-run the algorithm and continue to build the tree.</p>
<p>We need to search the distance matrix for <strong>the next two
closest</strong> points, <strong><em>ignoring</em></strong> the
<strong>first two</strong> that we already merged.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>nextmin <span class="ot">&lt;-</span> rdistxy[<span class="fu">order</span>(rdistxy)][<span class="dv">3</span>]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(rdistxy <span class="sc">==</span> nextmin, <span class="at">arr.ind =</span> <span class="cn">TRUE</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>ind</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>   row col</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span>  <span class="dv">11</span>  <span class="dv">10</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span>  <span class="dv">10</span>  <span class="dv">11</span></span></code></pre></div>
<p>Now we can plot the data with this next pair of points and the merged
tree leaves.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the points with the minimum overlayed</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[<span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)], y[<span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)], <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[ind[<span class="dv">1</span>, ]], y[ind[<span class="dv">1</span>, ]], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Make dendogram plots</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>hcluster <span class="ot">&lt;-</span> <span class="fu">dist</span>(dataFrame) <span class="sc">%&gt;%</span> hclust</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>dendro <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span>(hcluster)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>cutDendro <span class="ot">&lt;-</span> <span class="fu">cut</span>(dendro, <span class="at">h =</span> (hcluster<span class="sc">$</span>height[<span class="dv">2</span>]))</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cutDendro<span class="sc">$</span>lower[[<span class="dv">10</span>]], <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cutDendro<span class="sc">$</span>lower[[<span class="dv">5</span>]], <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-8-1.png" width="864" /></p>
<p>And on and on in this manner.</p>
</div>
<div id="stopping-the-algorithm." class="section level3" number="1.3.5">
<h3><span class="header-section-number">1.3.5</span> Stopping the
algorithm.</h3>
<p>So, hierarchical clustering is about finding the closest two points
and put them together in one cluster, then find the next closest pair in
the updated picture, and so forth.</p>
<p>We’ll repeat this process <strong>until we reach a reasonable
stopping place</strong>.</p>
<p>What does <strong><em>reasonable</em></strong> mean?</p>
<ul>
<li>There’s a lot of flexibility in this field and how we perform our
analysis depends on our problem.<br />
</li>
<li>Wikipedia tells us: <em>“one can decide to stop clustering either
when the clusters</em> <em>are too far apart to be merged</em>
<strong><em>(distance criterion)</em></strong> <em>or when there is
a</em> <em>sufficiently small number of clusters</em>
<strong><em>(number criterion)</em></strong>.</li>
</ul>
</div>
</div>
<div id="the-entire-tree-dendrogram-at-once-with-hclust."
class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> The entire tree
(dendrogram) at once, with <code>hclust()</code>.</h2>
<p>If we were to continue in this fashion –identifying the two closest
points and merging them-, we’d end up with a <em>dendrogram</em> that
looks like the one that follows.</p>
<p>In fact, that’s what the <code>hclust()</code> function we ran
before, does at once:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(hclust)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>List of <span class="dv">7</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> merge      <span class="sc">:</span> int [<span class="dv">1</span><span class="sc">:</span><span class="dv">11</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="sc">-</span><span class="dv">5</span> <span class="sc">-</span><span class="dv">10</span> <span class="sc">-</span><span class="dv">9</span> <span class="dv">2</span> <span class="sc">-</span><span class="dv">7</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">-</span><span class="dv">8</span> <span class="dv">6</span> <span class="dv">4</span> ...</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> height     <span class="sc">:</span> num [<span class="dv">1</span><span class="sc">:</span><span class="dv">11</span>] <span class="fl">0.0815</span> <span class="fl">0.0832</span> <span class="fl">0.1085</span> <span class="fl">0.208</span> <span class="fl">0.2167</span> ...</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> order      <span class="sc">:</span> int [<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>] <span class="dv">2</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">10</span> <span class="dv">11</span> <span class="dv">9</span> <span class="dv">12</span> <span class="dv">8</span> <span class="dv">7</span> ...</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> labels     <span class="sc">:</span> <span class="cn">NULL</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> method     <span class="sc">:</span> chr <span class="st">&quot;complete&quot;</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> call       <span class="sc">:</span> language <span class="fu">hclust</span>(<span class="at">d =</span> .)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> dist.method<span class="sc">:</span> chr <span class="st">&quot;euclidean&quot;</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a> <span class="sc">-</span> <span class="fu">attr</span>(<span class="sc">*</span>, <span class="st">&quot;class&quot;</span>)<span class="ot">=</span> chr <span class="st">&quot;hclust&quot;</span></span></code></pre></div>
<ul>
<li>It takes as an argument the <em>pairwise distance matrix</em> which
we looked at before.<br />
</li>
<li>It applies the the <em>clustering algorithm</em> and returns the
corrisponding <em>dendrogram</em>.<br />
</li>
<li>As we’ll see below, there are <em>different agglomeration
methods</em> that can be specified through the <code>method</code>
parameter, as one of:
<ul>
<li>ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA),
“mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).<br />
</li>
<li>The <em>default</em> grouping method is <em>complete</em>.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>hclust <span class="ot">&lt;-</span> <span class="fu">data.frame</span> (<span class="at">x =</span> x, <span class="at">y =</span> y) <span class="sc">%&gt;%</span> dist <span class="sc">%&gt;%</span> hclust</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>hclust</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>Call<span class="sc">:</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">hclust</span>(<span class="at">d =</span> .)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>Cluster method   <span class="sc">:</span> complete </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>Distance         <span class="sc">:</span> euclidean </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>Number of objects<span class="sc">:</span> <span class="dv">12</span> </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hclust)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<ul>
<li>The plot conveniently labeled everything for us.<br />
</li>
<li>The <strong>points</strong> we saw are the <strong>leaves at the
bottom</strong> of the graph.<br />
</li>
<li>Points 5 and 6 are connected, as are 10 and 11. Moreover, we see
that the original 3 groupings of points are closest together as leaves
on the picture. In other words, from the tree/dendrogram it’s clear that
there are <em>three clusters</em> each with <em>four points</em>.</li>
</ul>
<div id="how-to-determine-how-many-clusters-are-there"
class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> How to determine
how many clusters are there?</h3>
<p>One of the things about the dendrogram that’s produced by the
clustering algorithm is that <strong>it doesn’t actually tell us how
many clusters</strong> there are.</p>
<ul>
<li>There’s <em>no specific label</em> on the plot that tells us there
are <em>two</em> clusters or <em>three</em> clusters or whatever.</li>
</ul>
<p>And so, <strong>we have to “cut”</strong> this tree at a certain
point <strong>to determine how many</strong> <strong>clusters</strong>
there are.</p>
<ul>
<li>For example, if we cut it at the point labeled 2.0 on the y-axis or,
in other terms, if we draw a horizontal line at the <em>hight of
2.0</em>, then we determine that there are <em>two branches</em>, and
that would indicate us that there are roughly <em>two
clusters</em>.<br />
</li>
<li>However, if we draw a horizontal line at the <em>height of 1.0</em>,
then we would see that we run into <em>three branches</em>, so that
would tell us that there are roughly <em>three clusters</em>.<br />
</li>
<li>So depending on where we want to draw that horizontal line at the
tree we’ll get <em>more or fewer clusters</em> in our clustering.
<ul>
<li>Of course, in the <em>extreme case</em> if we were to cut the tree
at the bottom we’d just get 12 clusters, which equals the number of data
points.<br />
</li>
</ul></li>
<li>In summary, we have to <strong>cut the tree</strong> at a <em>place
that’s convenient for us</em>. We <strong>don’t really have a
rule</strong> of where to cut it but, once we do cut it, then we can get
the <em>cluster assignment</em> from hierarchical clustering.</li>
</ul>
</div>
<div
id="adjusting-a-bit-the-plotting-of-the-dendrogram-and-cutting-it-with-horizontal-lines."
class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Adjusting a bit the
plotting of the dendrogram and cutting it with horizontal lines.</h3>
<p>We can “adjust” a little bit the dendrogram plot, by doing like
this:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.dendrogram</span>(hclust))</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<ul>
<li>The essentials are the same, but the <em>labels are missing</em> and
the <em>leaves</em> (original points) are all printed at the <em>same
level</em>.<br />
</li>
<li>Notice that the vertical heights of the lines and labeling of the
scale on the left edge give some indication of distance.</li>
</ul>
<p>And we can use <code>abline()</code> command to cut (draw a
horizontal line) at different hights or distances (1.5, 0.4 and 0.05) on
this plot.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.dendrogram</span>(hclust))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">1.5</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">0.4</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">0.05</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<ul>
<li>We see that the blue line intersects 3 vertical lines and this tells
us that using the distance 1.5 gives us <em>3 clusters</em>:
<ul>
<li>1 through 4,<br />
</li>
<li>9 through 12, and<br />
</li>
<li>5 through 8.<br />
</li>
</ul></li>
<li>By cutting at 0.4 we have <em>5 clusters</em>, indicating that this
<strong>distance is small</strong> <strong>enough to break up</strong>
our <strong>original grouping</strong> of points.<br />
</li>
<li>The shortest distance between two points was around 0.08, so cutting
at distance smaller than that (like the green line at 0.05) would make
all the points their own private clusters.</li>
</ul>
</div>
<div id="the-heights-of-horizontal-lines-in-a-dendrogram-are-distances."
class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> The heights of
horizontal lines in a dendrogram are distances.</h3>
<p>If we look at the <strong>height of the horizontal lines</strong>
that are part of the graph (dendrogram), we can see right away the
<strong>distance between the two clusters</strong> <strong>(or
“points”)</strong> that are <strong>linked together</strong> by that
<strong>horizontal line</strong>.</p>
<ul>
<li>As we know, <em>that distance</em> is, by default, the distance
determined by the <em>complete linkage</em> method of
agglomeration.</li>
</ul>
<p>In our example, for instance:</p>
<ul>
<li><p>The <strong>distance between points 5 and 6</strong> is arround
<strong>0.8</strong>, which is the height of the horizontal line that
links those two points together.</p></li>
<li><p>Similarly the <strong>cluster of points 5 through 8</strong> and
the <strong>cluster of points 9</strong> <strong>through 12</strong> are
<strong>connected by a horizontal line that has a height of about
1.8</strong>, so <strong>that’s the (“complete”) distance</strong> that
separates those clusters.</p>
<ul>
<li>In fact, we can see in the distances matix that we calculated before
that the distance between the two farthest points of these clusters
(points 8 and 11) is 1.86999431.</li>
</ul></li>
</ul>
</div>
</div>
<div id="prettier-dendrograms." class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Prettier
dendrograms.</h2>
<div id="a-custom-function-that-add-colors-to-the-output-of-hclust."
class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> A custom function
that add colors to the output of <code>hclust()</code>.</h3>
<p>It’s possible to make slightly prettier dendrograms with some
<strong>modification to the</strong> <strong>usual <code>plot()</code>
method</strong> for the output of <code>hclust()</code>.</p>
<ul>
<li>There’s a <code>plclust()</code> function, but it’s been deprecated
for a long time.</li>
</ul>
<p>Here’s a function that takes the output of <code>hclust()</code> and
<em>color codes each of the</em> <em>leafs</em> (each of the <em>cluster
members</em> by their <em>cluster membership</em>).</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>myplclust <span class="ot">&lt;-</span> <span class="cf">function</span>(hclust, <span class="at">lab =</span> hclust<span class="sc">$</span>labels,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">lab.col =</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">length</span>(hclust<span class="sc">$</span>labels)),</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">hang =</span> <span class="fl">0.1</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                      ...){</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Modifiction of plot() for plotting hclust objects *in colour*!</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Copyright Eva KF Chan 2009</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Arguments:</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="do">##    hclust:    hclust object</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="do">##    lab:       a character vector of labels of the leaves of the tree</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="do">##    lab.col:   colour for the labels; NA=default device foreground colour</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="do">##    hang:      as in hclust &amp; plclust (the fraction of the plot height by</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="do">##               which labels should hang below the rest of the plot; a negative</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="do">##               value will cause the labels to hang down from 0)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Side effect:</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>  <span class="do">##    A display of hierarchical cluster with coloured leaf labels.</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rep</span>(hclust<span class="sc">$</span>height, <span class="dv">2</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(hclust<span class="sc">$</span>merge)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> y[<span class="fu">which</span>(x <span class="sc">&lt;</span> <span class="dv">0</span>)]</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> x[<span class="fu">which</span>(x <span class="sc">&lt;</span> <span class="dv">0</span>)]</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">abs</span>(x)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> y[<span class="fu">order</span>(x)]</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> x[<span class="fu">order</span>(x)]</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(hclust, <span class="at">labels =</span> <span class="cn">FALSE</span>, <span class="at">hang =</span> hang, ...)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text</span>(<span class="at">x =</span> x, <span class="at">y =</span> y[hclust<span class="sc">$</span>order]<span class="sc">-</span>(<span class="fu">max</span>(hclust<span class="sc">$</span>height)<span class="sc">*</span>hang),</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">labels =</span> lab[hclust<span class="sc">$</span>order],</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> lab.col[hclust<span class="sc">$</span>order],</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">srt =</span> <span class="dv">90</span>, <span class="at">adj =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>), <span class="at">xpd =</span> <span class="cn">NA</span>, ...)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Let’s apply this function to our example:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>hclust <span class="ot">&lt;-</span> <span class="fu">data.frame</span> (<span class="at">x =</span> x, <span class="at">y =</span> y) <span class="sc">%&gt;%</span> dist <span class="sc">%&gt;%</span> hclust</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">myplclust</span>(hclust, <span class="at">lab =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> <span class="dv">4</span>), <span class="at">lab.col =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> <span class="dv">4</span>))</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="a-lot-of-prettier-dendrograms-sources." class="section level3"
number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> A lot of prettier
dendrograms sources.</h3>
<p>We can find a lot of dendrograms functions the following sources:</p>
<ul>
<li><p><a
href="https://www.r-graph-gallery.com/31-custom-colors-in-dendrogram.html">The
R Graph Gallery</a></p></li>
<li><p>The <code>ggdendro</code> package from CRAN can be used to
<strong>extract the plot data from dendrogram</strong> and for
<strong>drawing a dendrogram using
<code>ggplot2</code></strong>.</p></li>
<li><p>The <code>dendextend</code> package contains many functions for
changing the appearance of a dendrogram and for comparing
dendrograms.</p></li>
</ul>
</div>
</div>
<div id="the-agglomeration-methods." class="section level2"
number="1.6">
<h2><span class="header-section-number">1.6</span> The agglomeration
methods.</h2>
<p>Let’s discuss <em>how exactly</em> the <em>merging of clusters
works</em>.</p>
<ul>
<li><p>Recall that once we find the two points that are closest
together, we “merge” them and then consider the merged pair as a single
“point”.</p></li>
<li><p>When we compare this merged “point” with other points,
<strong>how should we measure</strong> the <strong>distance from one
point to this merged cluster of points?</strong></p></li>
</ul>
<div id="complete-linkage." class="section level3" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> Complete
linkage.</h3>
<p>One method, called “complete”, is to measure the distance between two
groups of points <strong>by the maximun distance</strong> between the
two groups.</p>
<ul>
<li><p>That is, take all points in group 1 and all points in group 2 and
<em>find</em> the <em>two points that are furthest apart</em>.</p></li>
<li><p><em>That’s the distance</em> between the groups.</p></li>
</ul>
<p>Here’s what that would look like with our simulated data.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>dataFrame <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[<span class="dv">8</span>], y[<span class="dv">8</span>], <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[<span class="dv">1</span>], y[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(x[<span class="dv">8</span>], y[<span class="dv">8</span>], x[<span class="dv">1</span>], y[<span class="dv">1</span>], <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="average-linkage." class="section level3" number="1.6.2">
<h3><span class="header-section-number">1.6.2</span> Average
linkage.</h3>
<p>Another approach is “average” merging, which:</p>
<ul>
<li>takes the <strong>average of the coordinate values</strong> (which
we can think of the <em>center of gravity</em>) in <strong>each
group</strong>, and<br />
</li>
<li>measures the <strong>distance between these two averages</strong>
(two <em>centers of gravity</em>).</li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>dataFrame <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">mean</span>(x[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]), <span class="fu">mean</span>(y[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]), <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">mean</span>(x[<span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>]), <span class="fu">mean</span>(y[<span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>]), <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="fu">mean</span>(x[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]), <span class="fu">mean</span>(y[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]), <span class="fu">mean</span>(x[<span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>]), <span class="fu">mean</span>(y[<span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>]), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="which-method-to-choose" class="section level3" number="1.6.3">
<h3><span class="header-section-number">1.6.3</span> Which method to
choose?</h3>
<p>We can see that in the <em>complete linkage example</em> that
distance is relatively far, whereas in the <em>average linkage
example</em> the distance is somewhat shorter.</p>
<p>There’s <strong>not a right or wrong method</strong> or way to do the
merging.</p>
<ul>
<li>The point is to show that each of the two different merging
approaches can get to very different results.<br />
</li>
<li>So it’s is often <strong>useful to try both</strong> approaches to
see what kind of clustering results we get in the end and whether one
set makes more sense than another.</li>
</ul>
</div>
</div>
<div id="the-heatmap-function." class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> The
<code>heatmap()</code> function.</h2>
<p>The <code>heatmap()</code> function is a <strong>handy way to
visualize matrix data</strong>.</p>
<ul>
<li>If we have an <em>extremely large table o matrix of numbers</em>
that are <em>similarly</em> <em>scaled</em> and we want to <em>just take
a look at them</em> really quickly in an <em>organized way</em>, we can
call the <code>heatmap()</code> function.</li>
</ul>
<p>In general, <a
href="http://en.wikipedia.org/wiki/Heat_map">Wikipedia</a> tells us that
a <strong>heat map</strong> is <em>“a graphical representation of data
where the individual values<em> </em>contained in a matrix are
represented as colors …Heat maps originated in 2D<em> </em>displays of
the values in a data matrix. Larger values were represented by small<em>
</em>dark gray or black squares (pixels) and smaller values by lighter
squares”</em>.</p>
<p>What the <code>heatmap()</code> function does is <strong>essentially
runs a hierarchical cluster</strong> <strong>analysis</strong> on the
<em>rows</em> of the table and on the <em>columns</em> of the table:</p>
<ul>
<li><p>It <em>sorts</em> the <em>rows and columns</em> of a matrix
<em>according to the clustering</em> determined by a <em>call to
<code>hclust()</code></em>.</p></li>
<li><p>It <em>first treats the rows</em> of a matrix <em>as
observations</em> and calls <code>hclust()</code> on them.</p></li>
<li><p>Then it <em>treats the columns</em> of a matrix <em>as
observations</em> and calls <code>hclust()</code> on those
values.</p></li>
<li><p>The <em>end result</em> is that we get a <em>dendrogram</em>
<strong>associated with both</strong> the <em>rows</em> and
<em>columns</em> of a matrix, which can help us to spot obvious patterns
in the data.</p></li>
</ul>
<p>In other words, <code>heatmap()</code> <strong>uses
<code>hclust()</code> to organize</strong> the <strong>rows</strong> and
the <strong>columns</strong> of the tables so that we can
<strong>visualize them</strong> in <strong>groups or blocks of</strong>
<strong>observations</strong> within the table using the
<code>image()</code> function.</p>
<ul>
<li>So it <strong>creates a image plot</strong>, reording the columns
and the rows of the table according to the hierarchical clustering
algorithm.</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>dataMatrix <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y) <span class="sc">%&gt;%</span> data.matrix</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">heatmap</span>(dataMatrix)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<ul>
<li>The rows and columns are grouped together as shown by colors.<br />
</li>
<li>Here we can see that for example, along the rows we’ve got a
dendrogram which shows that there are probably three clusters, and those
rows are all grouped together.
<ul>
<li>The top rows (labeled 5, 6 and 7) seem to be in the same group (same
colors) while 8 is next to them but colored differently.<br />
</li>
<li>This matches the dendrogram shown on the left edge.<br />
</li>
<li>Similarly, 9, 12, 11, and 10 are grouped together (row-wise) along
with 3 and 2.<br />
</li>
</ul></li>
<li>Then there are only two columns in the data frame. So it’s not
particularly interesting to do a hierarchical cluster analysis on that.
<ul>
<li>But if we had many many columns, we would reorganize the columns so
that the closer ones would be closer together and the farther ones would
be farther apart.</li>
</ul></li>
</ul>
<div id="a-short-tutorial-for-decent-heat-maps-in-r."
class="section level3" number="1.7.1">
<h3><span class="header-section-number">1.7.1</span> A short tutorial
for decent heat maps in R.</h3>
<p>A very nice concise tutorial on creating heatmaps in R exists at:</p>
<p><a
href="http://sebastianraschka.com/Articles/heatmaps_in_r.html#clustering"
class="uri">http://sebastianraschka.com/Articles/heatmaps_in_r.html#clustering</a></p>
</div>
</div>
<div id="final-notes-and-further-resources." class="section level2"
number="1.8">
<h2><span class="header-section-number">1.8</span> Final notes and
further resources.</h2>
<ul>
<li><p>Hierarchical clustering <strong>gives an idea of the
relationships</strong> <em>between</em>
<em>variables/observations</em>.</p>
<ul>
<li>We need to define a notion of what it means for two points to be
close or far apart.<br />
</li>
<li>And we have to have a merging strategy.<br />
</li>
<li>Given those two things, we can run the hierarchical clustering
algorithm and it will produce a cluster dendrogram, which will show us
how the merging is done or how the points got merged together through
the algorithm.</li>
</ul></li>
<li><p>The produced <strong>picture</strong> may be unstable, in
particular, may be <strong>sensitive to</strong>:</p>
<ul>
<li>Changing a few points in the dataset.
<ul>
<li>For example, if we have some outliers and we remove them or modify
them a little bit, then the clustering dendrogram could change a
lot.<br />
</li>
</ul></li>
<li>Having different missing values in some of the observations.<br />
</li>
<li>Picking a different distance metric (i.e. euclidean vs manhattan)
and changing the merging strategy (i.e. complete vs average).
<ul>
<li>Often a useful thing is to try different distance metrics and
changing merging strategy, to see how sensitive it is (how picture
changes) to the different distance metrics and merging methods.<br />
</li>
</ul></li>
<li>Changing the scale of points for one variable.
<ul>
<li>If one variable, for example, is measured on units that are much
larger than another variable, then that can sometimes throw off the
algorithm.<br />
</li>
<li>So, it may be useful to scale certain variables so that they’re
better comparable to each other.</li>
</ul></li>
</ul></li>
<li><p>Hierarchical clustering, at least the algorithm we’ve discussed
before, is <strong>deterministic</strong>.</p>
<ul>
<li>There’s no random starting point, there’s no randomness in it.<br />
</li>
<li>If we run it with the same parameters and the same data, it will
give us the same picture.</li>
</ul></li>
<li><p>Choosing <strong>where to “cut” the tree</strong> to
<em>determine the number of clusters</em> isn’t always obvious.</p>
<ul>
<li>A key question in any clustering approach is where to cut.<br />
</li>
<li>The general idea is determining how many clusters there are.<br />
</li>
<li>It’s not always obvious to figure out how many clusters there are. A
number of algorithms that has been proposed, to try to figure that
out.</li>
</ul></li>
<li><p>Should be <strong>primarily used for exploration</strong> of data
(once major patterns have been identified, it’s often best to delve
further with other tools and formal modeling).</p></li>
<li><p>Some other resoruces:</p>
<ul>
<li><a href="http://www.youtube.com/watch?v=wQhVWUcXM0A">Rafa’s
Distances and Clustering Video</a><br />
</li>
<li><a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">Elements
of statistical learning</a></li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="k-means-culstering." class="section level1" number="2">
<h1><span class="header-section-number">2</span> K-Means
Culstering.</h1>
<p>The K-means clustering algorithm is <em>another bread-and-butter
algorithm</em> in <strong>high-</strong> <strong>dimensional data
analysis</strong>, that dates back many decades now.</p>
<ul>
<li><p>The K-means approach, like many clustering methods, is <em>highly
algorithmic</em> (can’t be summarized in a formula) and is
<em>iterative</em>.</p></li>
<li><p>And, as a <em>clustering</em> procedure, it’s essentially useful
to determine what kinds of observations are similar to each other and
what kind of observations are very different from each other.</p></li>
</ul>
<div id="the-three-aspects-of-k-means-clustering."
class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> The three aspects of
k-means clustering.</h2>
<div id="what-is-it-a-partitioning-approach." class="section level3"
number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> What is it: a
partitioning approach.</h3>
<p>The K-means approach is <strong>a partitioning approach</strong>,
whereby the data (observations) are partitioned into groups at each
iteration of the algorithm.</p>
<ul>
<li>In short, a way of <strong>partitioning</strong> a group of
observations <strong>into a fixed number</strong> of
<strong>clusters</strong>.</li>
</ul>
<p>The <strong>basic idea</strong> is that we are trying to <em>find the
centroids</em> of a <em>fixed</em> number of <em>clusters of points</em>
in a <em>high-dimensional space</em>.</p>
<ul>
<li>In <em>two dimensions</em>, we can imagine that there are a bunch of
clouds of points on the plane and we want to figure out where the
centers of each one of those clouds is.<br />
</li>
<li>Of course, in two dimensions, we could probably just look at the
data and figure out with a high degree of accuracy where the cluster
centroids are.<br />
</li>
<li>But what if the data are in a <em>100-dimensional space?</em> That’s
where we need an <strong>algorithm</strong>.</li>
</ul>
<p>The <strong>outline of the algorithm</strong> is:</p>
<ol style="list-style-type: decimal">
<li>Fix the number of clusters at some integer greater than or equal to
2.<br />
</li>
<li>Start with the “centroids” of each cluster.
<ul>
<li>Initially we might just pick a random set of “phantom points” as the
centroids.<br />
</li>
</ul></li>
<li>Assign points to their closest centroid (cluster membership
corresponds to the centroid assignment).
<ul>
<li>R documentation tells us that the k-means method <em>aims to
partition the points</em> <em>into k groups such that</em>
<strong><em>the sum of squares from points to the assigned</em></strong>
<strong><em>cluster centres is minimized</em></strong>”.<br />
</li>
</ul></li>
<li>Reclaculate centroid positions by making it the average of the
points assigned to each centroid and repeat from step 3.</li>
</ol>
</div>
<div
id="what-does-it-require-a-distance-metric-a-fixed-number-of-clusters-and-an-initial-centroids-guess."
class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> What does it
require: a distance metric, a fixed number of clusters and an initial
centroids guess.</h3>
<p>This approach, like most clustering methods requires:</p>
<ul>
<li>A defined <em>distance metric</em>.<br />
</li>
<li>A <em>fixed number of clusters</em>.
<ul>
<li>Of course, this may not be known in advance, but we can guess and
just run the algorithm anyway.<br />
</li>
<li>Afterwards, we can change the number of clusters and run the
algorithm again to see if anything changes.<br />
</li>
<li>There are though a number of algorithms to pick the number of
clusters up: cross validation/information theory, etc.<br />
</li>
</ul></li>
<li>And an <em>initial guess</em> <strong><em>as to the cluster
centriods</em></strong>.
<ul>
<li>There’s no set approach to determining the initial configuration of
centroids, but <em>many algorithms simply randomly select</em> data
points from your dataset as the initial centroids.</li>
</ul></li>
</ul>
</div>
<div
id="what-does-it-produce-final-estimate-of-cluster-centroids-and-points-assignment"
class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> What does it
produce: final estimate of cluster centroids and points assignment</h3>
<p>The K-means algorithm produces:</p>
<ul>
<li>A final estimate of cluster centroids (i.e. their
coordinates).<br />
</li>
<li>An assignment of each point to their respective clusters.</li>
</ul>
</div>
</div>
<div id="k-means-clustering-example." class="section level2"
number="2.2">
<h2><span class="header-section-number">2.2</span> K-means clustering
example.</h2>
<p>Let’s <strong>simulate</strong> the same data we used as running
example before with hierarchical clustering, that is, some data in
<strong>three separate clusters</strong>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">12</span>, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> <span class="dv">4</span>), <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">12</span>, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">each =</span> <span class="dv">4</span>), <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div id="first-step-starting-centroids." class="section level3"
number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> First step:
starting centroids.</h3>
<p>The first thing K-means has to do is <strong>assign an initial set of
centroids</strong>.</p>
<p>For this example:</p>
<ul>
<li>We will <strong>assume</strong> that there are three clusters (which
also happens to be the <em>truth</em>).<br />
</li>
<li>We will <em>choose three centroids arbitrarily</em> and show them in
the plot below.</li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="fl">2.5</span>, <span class="dv">4</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>,<span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>cx <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.8</span>, <span class="fl">2.5</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>cy <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">1.5</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(cx, cy, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;purple&quot;</span>), <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-19-1.png" width="576" /></p>
</div>
<div id="assigning-points-to-closest-centroid." class="section level3"
number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Assigning points to
closest centroid.</h3>
<p>The next stage in the algorithm <strong>assigns every point</strong>
in the dataset to the <strong>closest centroid</strong>.</p>
<ul>
<li><p>To do that, we have to calculate the <strong>(Euclidean)
distances</strong>:</p>
<ul>
<li>12 points x 3 centroids = 36 distances</li>
</ul></li>
</ul>
<p>In the plot below, we color each point according to the color of its
closest centroid (red, purple or orange).</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="fl">2.5</span>, <span class="dv">4</span>))</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>cols1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>cx <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.8</span>, <span class="fl">2.5</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>cy <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">1.5</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(cx, cy, <span class="at">col =</span> cols1, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Find the closest centroid</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="do">### Find the distance of each of 12 points from each of 3 centroids</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="do">### (store the results in a 3 x 12 matrix, the rows are the centroids </span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="do">###  and the columns are the points).</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>distTmp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">12</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">2</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">3</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>distTmp</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>         [,<span class="dv">1</span>]      [,<span class="dv">2</span>]      [,<span class="dv">3</span>]     [,<span class="dv">4</span>]      [,<span class="dv">5</span>]      [,<span class="dv">6</span>]      [,<span class="dv">7</span>]</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="fl">1.392885</span> <span class="fl">0.9774614</span> <span class="fl">0.7000680</span> <span class="fl">1.264693</span> <span class="fl">1.1894610</span> <span class="fl">1.2458771</span> <span class="fl">0.8113513</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,] <span class="fl">1.108644</span> <span class="fl">0.5544675</span> <span class="fl">0.3768445</span> <span class="fl">1.611202</span> <span class="fl">0.8877373</span> <span class="fl">0.7594611</span> <span class="fl">0.7003994</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,] <span class="fl">3.461873</span> <span class="fl">2.3238956</span> <span class="fl">1.7413021</span> <span class="fl">4.150054</span> <span class="fl">0.3297843</span> <span class="fl">0.2600045</span> <span class="fl">0.4887610</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>         [,<span class="dv">8</span>]      [,<span class="dv">9</span>]     [,<span class="dv">10</span>]     [,<span class="dv">11</span>]     [,<span class="dv">12</span>]</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="fl">1.026750</span> <span class="fl">4.5082665</span> <span class="fl">4.5255617</span> <span class="fl">4.8113368</span> <span class="fl">4.0657750</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,] <span class="fl">2.208006</span> <span class="fl">1.1825265</span> <span class="fl">1.0540994</span> <span class="fl">1.2278193</span> <span class="fl">1.0090944</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,] <span class="fl">1.337896</span> <span class="fl">0.3737554</span> <span class="fl">0.4614472</span> <span class="fl">0.5095428</span> <span class="fl">0.2567247</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="do">### Assign a cluster to each point (look at each column and pick the</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="do">### minimum entry)</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>newClust <span class="ot">&lt;-</span> <span class="fu">apply</span>(distTmp, <span class="dv">2</span>, which.min)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>newClust</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> cols1[newClust])</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-20-1.png" width="576" /></p>
<ul>
<li>So this is the <em>initial</em> grouping of the points to the three
different clusters.<br />
</li>
<li>We can see that this <em>initial</em> clustering <strong>incorrectly
clusters some points</strong> that are truly in the same cluster to
separate clusters.<br />
</li>
<li>The <em>hope</em> is that <em>iterating</em> algorithm more times
that we will eventually <em>converge on the correct solution</em>.</li>
</ul>
</div>
<div id="recalculating-centroids." class="section level3"
number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Recalculating
centroids.</h3>
<p>The next stage is <strong>re-calculate the centroids</strong> based
on the new cluster assignments of the data points.</p>
<ul>
<li>Once we every point is assigned to a cluster, we can recalculate the
centroids for <em>example</em> by <em>taking the mean</em> of that
cluster.</li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="fl">2.5</span>, <span class="dv">4</span>))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>cols1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Find the closest centroid</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>distTmp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">12</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">2</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">3</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>newClust <span class="ot">&lt;-</span> <span class="fu">apply</span>(distTmp, <span class="dv">2</span>, which.min)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> cols1[newClust])</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Old centroids</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>cx <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.8</span>, <span class="fl">2.5</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>cy <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">1.5</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="do">## New centroids</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>newCx <span class="ot">&lt;-</span> <span class="fu">tapply</span>(x, newClust, mean)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>newCx</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>       <span class="dv">1</span>        <span class="dv">2</span>        <span class="dv">3</span> </span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="fl">1.210767</span> <span class="fl">1.010320</span> <span class="fl">2.498011</span> </span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>newCy <span class="ot">&lt;-</span> <span class="fu">tapply</span>(y, newClust, mean)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>newCy</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>       <span class="dv">1</span>        <span class="dv">2</span>        <span class="dv">3</span> </span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="fl">1.730555</span> <span class="fl">1.016513</span> <span class="fl">1.354373</span> </span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="do">## Plotting both ols and new centroids.</span></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(cx, cy, <span class="at">col =</span> cols1, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(newCx, newCy, <span class="at">col =</span> cols1, <span class="at">pch =</span> <span class="dv">8</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-21-1.png" width="576" /></p>
<ul>
<li>In fact, by taking the mean, we can see that the purple pluses has
moved slightly to be in the middle of that cluster. The red plus has
moved a little bit to be in the middle of that cluster. And the orange
one is now in the middle of the three orange dots.</li>
</ul>
</div>
<div id="reassigning-values-and-updating-centroids."
class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Reassigning values
and updating centroids.</h3>
<p>Now we have completed one full cycle of the algorithm, we can
continue and re-assign points to their (new) closest cluster
centroid.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="fl">2.5</span>, <span class="dv">4</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>cols1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Find the closest centroid</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>distTmp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">12</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">2</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">3</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>newClust <span class="ot">&lt;-</span> <span class="fu">apply</span>(distTmp, <span class="dv">2</span>, which.min)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>newClust</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Old centroids</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>cx <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.8</span>, <span class="fl">2.5</span>)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>cy <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">1.5</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="do">## New centroids</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>newCx <span class="ot">&lt;-</span> <span class="fu">tapply</span>(x, newClust, mean)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>newCy <span class="ot">&lt;-</span> <span class="fu">tapply</span>(y, newClust, mean)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(newCx, newCy, <span class="at">col =</span> cols1, <span class="at">pch =</span> <span class="dv">8</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Iteration 2</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>distTmp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">12</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>newCx[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>newCy[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">2</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>newCx[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>newCy[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">3</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>newCx[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>newCy[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>newClust2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(distTmp, <span class="dv">2</span>, which.min)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>newClust2</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, y , <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> cols1[newClust2])</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-22-1.png" width="576" /></p>
<p>And we can <strong>update the centroid positions</strong> one more
time based on the re-assigned points.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="fl">2.5</span>, <span class="dv">4</span>))</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>cols1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(x <span class="sc">+</span> <span class="fl">0.05</span>, y <span class="sc">+</span> <span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Find the closest centroid</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>distTmp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">12</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">2</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">3</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>cx[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>cy[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>newClust <span class="ot">&lt;-</span> <span class="fu">apply</span>(distTmp, <span class="dv">2</span>, which.min)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>newClust</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Old centroids</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>cx <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.8</span>, <span class="fl">2.5</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>cy <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">1.5</span>)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(cx, cy, <span class="at">col =</span> cols1, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="do">## New Old centroids</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>newCx <span class="ot">&lt;-</span> <span class="fu">tapply</span>(x, newClust, mean)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>newCy <span class="ot">&lt;-</span> <span class="fu">tapply</span>(y, newClust, mean)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(newCx, newCy, <span class="at">col =</span> cols1, <span class="at">pch =</span> <span class="dv">8</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Iteration 2</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>distTmp <span class="ot">&lt;-</span> <span class="fu">matrix</span>( <span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">12</span>)</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>newCx[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>newCy[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">2</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>newCx[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>newCy[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>distTmp[<span class="dv">3</span>, ] <span class="ot">&lt;-</span> (x<span class="sc">-</span>newCx[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (y<span class="sc">-</span>newCy[<span class="dv">3</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>finalClust <span class="ot">&lt;-</span> <span class="fu">apply</span>(distTmp, <span class="dv">2</span>, which.min)</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>finalClust</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a><span class="do">## Final centroids</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>finalCx <span class="ot">&lt;-</span> <span class="fu">tapply</span>(x, finalClust, mean)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>finalCy <span class="ot">&lt;-</span> <span class="fu">tapply</span>(y, finalClust, mean)</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(finalCx, finalCy, <span class="at">col =</span> cols1, <span class="at">pch =</span> <span class="dv">9</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> cols1[finalClust])</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-23-1.png" width="576" /></p>
<ul>
<li>We can see from this last plot that things are actually pretty close
to where they should be.<br />
</li>
<li>There are just two purple points that have been assigned to the
wrong cluster.</li>
</ul>
</div>
<div id="stopping-the-algorithm.-1" class="section level3"
number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Stopping the
algorithm.</h3>
<p>In practice, <strong>we would not know</strong> where the actual
clusters were, so we wouldn’t necessarily know when we were close to the
truth.</p>
<p>But eventually our algorithm needs to stop, so <strong>how do we
decide when to stop</strong> iterating?</p>
<ul>
<li>At some point the <em>cluster centroids will</em> stabilize and
<em>stop moving</em> with each iteration.<br />
</li>
<li>We could see that, from the first iteration to the second iteration,
the cluster centroids moved a lot. But after the second iteration, they
moved less.<br />
</li>
<li>Between each iteration <strong>we can keep track of the
distance</strong> that each centroid moves from one iteration to the
next.<br />
</li>
<li>Once this distance is relatively small, <em>we can stop</em> the
algorithm.</li>
</ul>
<p>So the process stops once we <strong>reach an iteration in which no
adjustments are made</strong> or when we’ve <strong>reached some
predetermined maximum number of iterations</strong>.</p>
</div>
<div id="the-algorithm-at-once-with-kmeans-function."
class="section level3" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> The algorithm at
once with <code>kmeans()</code> function.</h3>
<p>The <code>kmeans()</code> function in R implements the K-means
algorithm.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(kmeans)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span> (x, centers, <span class="at">iter.max =</span> 10L, <span class="at">nstart =</span> 1L, <span class="at">algorithm =</span> <span class="fu">c</span>(<span class="st">&quot;Hartigan-Wong&quot;</span>, </span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Lloyd&quot;</span>, <span class="st">&quot;Forgy&quot;</span>, <span class="st">&quot;MacQueen&quot;</span>), <span class="at">trace =</span> <span class="cn">FALSE</span>)  </span></code></pre></div>
<ul>
<li><code>x</code> is a matrix or data frame of data.
<ul>
<li>The data should be organized so that each row is an observation and
each column is a variable or feature of that observation.<br />
</li>
</ul></li>
<li><code>centers</code> is either:
<ul>
<li><em>an integer</em> indicating the number of clusters, or<br />
</li>
<li><em>or a matrix</em> indicating the locations of the initial cluster
centroids.<br />
</li>
</ul></li>
<li><code>iter.max</code> specifies the maximum number of iterations to
go through.<br />
</li>
<li><code>nstart</code> is the number of random starts we want to try if
we specify centers as a number.</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>dataFrame <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>kmeansObj <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(dataFrame, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">2</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>kmeansObj</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>K<span class="sc">-</span>means clustering with <span class="dv">3</span> clusters of sizes <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>Cluster means<span class="sc">:</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>          x         y</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="fl">1.9906904</span> <span class="fl">2.0078229</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span> <span class="fl">2.8534966</span> <span class="fl">0.9831222</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span> <span class="fl">0.8904553</span> <span class="fl">1.0068707</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>Clustering vector<span class="sc">:</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>Within cluster sum of squares by cluster<span class="sc">:</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="fl">0.34732441</span> <span class="fl">0.03298027</span> <span class="fl">0.34188313</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a> (between_SS <span class="sc">/</span> <span class="at">total_SS =</span>  <span class="fl">93.6</span> %)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>Available components<span class="sc">:</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="st">&quot;cluster&quot;</span>      <span class="st">&quot;centers&quot;</span>      <span class="st">&quot;totss&quot;</span>        <span class="st">&quot;withinss&quot;</span>     <span class="st">&quot;tot.withinss&quot;</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>[<span class="dv">6</span>] <span class="st">&quot;betweenss&quot;</span>    <span class="st">&quot;size&quot;</span>         <span class="st">&quot;iter&quot;</span>         <span class="st">&quot;ifault&quot;</span>      </span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>kmeansObj<span class="sc">$</span>centers</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>          x         y</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="fl">1.9906904</span> <span class="fl">2.0078229</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span> <span class="fl">2.8534966</span> <span class="fl">0.9831222</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span> <span class="fl">0.8904553</span> <span class="fl">1.0068707</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>kmeansObj<span class="sc">$</span>cluster</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>kmeansObj<span class="sc">$</span>iter</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="dv">1</span></span></code></pre></div>
<ul>
<li><code>kmeans</code> returns the information that the data clustered
into 3 clusters each of size 4.<br />
</li>
<li>It also returns the coordinates of the 3 cluster means.<br />
</li>
<li>A vector named <code>cluster</code> indicating how the 12 points
were partitioned into the clusters.<br />
</li>
<li>The sum of squares within each cluster.<br />
</li>
<li><code>iter</code> shows how many iterations the algorithm went
through.
<ul>
<li><em>Two</em> iterations as we did before.</li>
</ul></li>
</ul>
<p>Here is a <strong>plot of the K-means clustering
solution</strong>.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">rep</span>(<span class="fl">2.5</span>, <span class="dv">4</span>))</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> kmeansObj<span class="sc">$</span>cluster, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(kmeansObj<span class="sc">$</span>centers, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-26-1.png" width="576" /></p>
<ul>
<li>Not surprisingly for this simple dataset, K-means was able to
identify the true solution.</li>
</ul>
</div>
</div>
<div id="building-heatmaps-from-k-means-solutions."
class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Building heatmaps
from K-means solutions.</h2>
<p>A heatmap or image plot is sometimes a useful way to
<strong>visualize matrix or array data</strong>.</p>
<p>The <strong>idea</strong> is that <em>each cell</em> of the image is
<em>colored in a manner proportional</em> to the <em>value</em> in the
corresponding matrix element.</p>
<p>It take a bit of work to get this to look right in R but the result
can be very useful, especially for high-dimensional datasets that can be
visualized using the simple plots we used above.</p>
<p>First, let’s <em>find a K-means solution</em> to a <em>different
dataset</em> obtained from our previous one after randomly reordering
the rows.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>dataMatrix <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dataFrame)[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>), ]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>kmeansObj <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(dataMatrix, <span class="at">centers =</span> <span class="dv">3</span>)</span></code></pre></div>
<p>Then we can <strong>make an <code>image()</code> plot</strong> using
the <em>K-means clusters</em>.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(dataMatrix)[ , <span class="fu">nrow</span>(dataMatrix)<span class="sc">:</span><span class="dv">1</span>], <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Original Data&quot;</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(dataMatrix)[ , <span class="fu">order</span>(kmeansObj<span class="sc">$</span>cluster)], <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Clustered Data&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<ul>
<li>The plot above <em>orders the rows</em> of the matrix/image so that
all of the rows in the <em>same cluster</em> are <em>grouped
together</em>.<br />
</li>
<li>We can see this with the more homogeneous nature of the coloring in
the clustered version of the image.</li>
</ul>
</div>
<div id="notes-and-further-resources." class="section level2"
number="2.4">
<h2><span class="header-section-number">2.4</span> Notes and further
resources.</h2>
<ul>
<li>K-means requires a number of clusters
<ul>
<li>Pick by eye/intuition</li>
<li>Pick by cross validation/information theory, etc.</li>
<li><a
href="http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set">Determining
the number of clusters</a></li>
</ul></li>
<li>K-means is not deterministic
<ul>
<li>Different number of clusters</li>
<li>Different number of iterations</li>
</ul></li>
<li><a href="http://www.youtube.com/watch?v=wQhVWUcXM0A">Rafael
Irizarry’s Distances and Clustering Video</a></li>
<li><a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">Elements
of statistical learning</a></li>
</ul>
<hr />
</div>
</div>
<div
id="dimension-reduction-singular-value-decomposition--svd--and-principal-components-analysis--pca-."
class="section level1" number="3">
<h1><span class="header-section-number">3</span> Dimension Reduction:
Singular Value Decomposition -SVD- and Principal Components Analysis
-PCA-.</h1>
<p><strong>Singular Value Decomposition</strong> (SVD) and
<strong>Principal Components Analysis</strong> (PCA) are a really
<em>important statistical dimension reduction techniques</em>.</p>
<ul>
<li><p>They can be applied both in the <em>exploratory</em> data
analysis <em>phase</em>, and also, in the <em>more formal modeling
phase</em>.</p></li>
<li><p>Here we’re going to see a little bit about how those techniques
are used in the exploratory phase and a bit about their underlying
basis.</p></li>
</ul>
<div
id="svd-and-pca-as-statistical-techniques-for-high-dimensional-matrix-data."
class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> SVD and PCA as
statistical techniques for high-dimensional matrix data.</h2>
<p>Matrix data have some <em>special statistical methods</em> that can
be applied to them.</p>
<ul>
<li><p><em>Principal components analysis</em> (PCA) and the <em>singular
value decomposition</em> (SVD) are one category of statistical dimension
reduction techniques that can be applied to matrices.</p></li>
<li><p>These techniques generally are applied in situations where the
rows of a matrix represent observations of some sort and the columns of
the matrix represent features or variables (but this is by no means a
requirement).</p></li>
</ul>
<p>Let’s recall that the <strong>key aspect of matrix data</strong> is
that every element of the matrix is the <strong>same type</strong> and
represents the <strong>same kind of measurement</strong>.</p>
<ul>
<li>This is in contrast to a data frame, where every column of a data
frame can potentially be of a different class.</li>
</ul>
<blockquote>
<p>In an abstract sense, the <strong>SVD or PCA</strong> can be thought
of as a <strong>way to approximate</strong> a <em>high-dimensional
matrix</em> with a a <em>few low-dimensional matrices</em>.</p>
</blockquote>
<p>So in this section we’ll se how this works.</p>
</div>
<div id="simulating-purely-random-matrix-data." class="section level2"
number="3.2">
<h2><span class="header-section-number">3.2</span> Simulating purely
random matrix data.</h2>
<p>Let’s begin simulating some <em>purely random</em> 40x10 matrix
data.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>dataMatrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">400</span>), <span class="at">nrow =</span> <span class="dv">40</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">40</span>, <span class="fu">t</span>(dataMatrix)[, <span class="fu">nrow</span>(dataMatrix)<span class="sc">:</span><span class="dv">1</span>])</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-29-1.png" width="480" /></p>
</div>
<div id="applying-hierarchical-clustering-to-explore-the-matrix-data."
class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Applying hierarchical
clustering to explore the matrix data.</h2>
<p>When confronted with matrix data a <strong>quick and easy thing to
organize the data</strong> a bit, is to apply a <strong>hierarchical
clustering algorithm</strong> to it.</p>
<p>As we know, such a clustering can be visualized with the
<code>heatmap()</code> function.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">heatmap</span>(dataMatrix)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-30-1.png" width="576" /></p>
<ul>
<li><p>Not surprisingly, <strong>there aren’t</strong> really
<strong>any interesting patterns</strong> given that we just simulated
<em>random noise</em>.</p></li>
<li><p>In other words, even with the clustering that heatmap provides,
permuting the rows (observations) and columns (variables) independently,
the data still looks random.</p></li>
<li><p>At least it’s <em>good to know</em> that the clustering algorithm
won’t pick up something when there’s nothing there!</p></li>
</ul>
</div>
<div id="adding-a-pattern-to-our-matrix-data-to-see-what-happens."
class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Adding a pattern to
our matrix data to see what happens.</h2>
<p>If our matrix data had a pattern, <strong>how would we discover
it?</strong></p>
<p>So let’s simulate some data that indeed does have a pattern.</p>
<ul>
<li>In the code below, we cycle through all the rows of the matrix and
<strong>randomly add</strong> <strong>a constant</strong> -3- to the
<strong>last 5 columns</strong> of the matrix.</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">678910</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">40</span>){</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># flip a coin</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  coinFlip <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fl">0.5</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># if coin is heads add a common pattern to that row</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>  pattern <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="at">each =</span> <span class="dv">5</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(coinFlip){</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    dataMatrix[i, ] <span class="ot">&lt;-</span> dataMatrix[i, ] <span class="sc">+</span> pattern</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span> (<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">40</span>, <span class="fu">t</span>(dataMatrix)[, <span class="fu">nrow</span>(dataMatrix)<span class="sc">:</span><span class="dv">1</span>])</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-31-1.png" width="480" /></p>
<ul>
<li>We can clearly see that some of the rows on the right side of the
matrix have higher values than on the left side.</li>
</ul>
</div>
<div id="applying-hierarchical-clustering-to-explore-the-matrix-data.-1"
class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Applying hierarchical
clustering to explore the matrix data.</h2>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">heatmap</span>(dataMatrix)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<ul>
<li>We can see from the dendrogram on top of the matrix (for the
columns) that the <strong>columns pretty clearly split into two
clusters</strong>, which is what we’d expect.<br />
</li>
<li>The <strong>rows</strong> still look random.</li>
</ul>
</div>
<div id="general-case-matrices-with-patterns-in-rows-and-columns."
class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> General case:
matrices with patterns in rows and columns.</h2>
<p>In general, with matrix data, <em>there may be patterns</em> that
occur <em>accross the rows</em> <strong><em>and</em></strong>
<em>columns</em> of the matrix.</p>
<p>We can take a closer look at the patterns by looking at the
<strong>marginal means</strong> of the rows and columns.</p>
<p>In the <strong>example above</strong>, we <em>shifted the mean</em>
of <em>some of the observations</em> in <em>columns 5 through
10</em>.</p>
<ul>
<li>We can <strong>display this a bit more explicitly</strong> by
looking at the <strong>row and column</strong> <strong>means</strong> of
the data.</li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>hh <span class="ot">&lt;-</span> <span class="fu">dist</span>(dataMatrix) <span class="sc">%&gt;%</span> hclust</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>hh<span class="sc">$</span>order</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] <span class="dv">33</span> <span class="dv">39</span> <span class="dv">20</span> <span class="dv">26</span> <span class="dv">32</span> <span class="dv">17</span>  <span class="dv">1</span> <span class="dv">19</span>  <span class="dv">2</span> <span class="dv">18</span>  <span class="dv">9</span> <span class="dv">24</span> <span class="dv">35</span> <span class="dv">25</span> <span class="dv">13</span>  <span class="dv">3</span> <span class="dv">23</span> <span class="dv">36</span> <span class="dv">37</span> <span class="dv">14</span> <span class="dv">31</span> <span class="dv">21</span> <span class="dv">10</span> <span class="dv">30</span>  <span class="dv">5</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>[<span class="dv">26</span>] <span class="dv">29</span>  <span class="dv">4</span> <span class="dv">34</span> <span class="dv">11</span>  <span class="dv">8</span> <span class="dv">15</span> <span class="dv">12</span> <span class="dv">28</span>  <span class="dv">7</span> <span class="dv">22</span> <span class="dv">27</span> <span class="dv">16</span> <span class="dv">40</span>  <span class="dv">6</span> <span class="dv">38</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>dataMatrixOrdered <span class="ot">&lt;-</span> dataMatrix[hh<span class="sc">$</span>order, ]</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Complete data.</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(dataMatrixOrdered)[, <span class="fu">nrow</span>(dataMatrixOrdered)<span class="sc">:</span><span class="dv">1</span>])</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Show the row means.</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">rowMeans</span>(dataMatrixOrdered), <span class="dv">40</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">xlab =</span> <span class="st">&quot;Row Mean&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Row&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Show the column means.</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">colMeans</span>(dataMatrixOrdered), <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Column Mean&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-33-1.png" width="1152" /></p>
<ul>
<li>In the middle plot, on the y-axis we’ve got the row number which
goes from 1 to 40, so that is roughly parallel with the image on the
left. And and on the x-axis we’ve got the mean of that row.<br />
</li>
<li>So for example, we can see that for row 10 the mean is roughly minus
0.25 or something like that. For row 30 the mean is roughly 1.5.<br />
</li>
<li>We see then that there’s a <strong>clear shift in the mean</strong>
as we go across the rows.<br />
</li>
<li>Similarly, if we go across the columns, we can see there’s a
<strong>clear shift</strong> <strong>in the mean</strong> of each
column: the first 5 columns have roughly a mean of zero or close to it
and then the next 5 columns have roughly a mean of two because, there’s
a shift there.<br />
</li>
<li>So using the plots in the middle and on the right, we can see a
clear pattern in the rows and the columns.</li>
</ul>
</div>
<div id="more-complex-situations." class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> More complex
situations.</h2>
<p>There may be other patterns <em>beyond a simple mean shift</em>.</p>
<ul>
<li>So more sophisticated methods will be needed.</li>
</ul>
<p>Futhermore, there may be <strong>multiple patterns layered</strong>
on top of each other.</p>
<ul>
<li>So we need a <strong>method that can distangle</strong> these
patterns.</li>
</ul>
</div>
<div id="related-problems." class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Related
problems.</h2>
<p>Cluster analysis is useful for identifying patterns, but we can maybe
take a little <strong>more formal approach</strong>, that <strong>takes
advantage of the matrix structure</strong> of the data.</p>
<p>There are <strong>two kinds of problems</strong> we might want to
look at.</p>
<div id="one-kind-of-problem--a-pca-kind-of-problem-"
class="section level3" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> One kind of problem
-a PCA kind of problem-</h3>
<p>Suppose we have multivariate observations</p>
<p><span class="math display">\[X_1,\ldots,X_n\]</span></p>
<p>so that each of the <em>n</em> observations has <em>m</em>
features</p>
<p><span class="math display">\[X_1 =
(X_{11},\ldots,X_{1m})\]</span></p>
<p>Given this setup, <strong>the goal is:</strong></p>
<ul>
<li><p>to find a <em>new set of variables/features</em> that are
<strong>uncorrelated</strong> and <strong>explain</strong> <em>as much
variance</em> in the data <em>as possible</em>.</p></li>
<li><p>This goal is <strong>statistical</strong> in nature and it’s a
common problem that’s <strong>solved by</strong> the method of
<strong>principle components analysis</strong> (PCA).</p></li>
</ul>
<p>Saying all that <strong>in plain words</strong>, the situation is
this:</p>
<ul>
<li>We have a <em>lot of different variables</em> (hundreds or maybe
thousands or tens of thousands of variables in our dataset) that are
<em>not all independent</em> measurements of something (a lot of them
will be related to each other, correlated with each other, for example,
two measurements that are like height and weight and so those will
obviously be related to each other).<br />
</li>
<li>And we want to <em>create a new set of variables</em> that is
<em>smaller</em> than the original set of variables and that its
variables are <em>uncorrelated</em> (represent different types of
variation in our dataset) and <em>explain as much</em> variance as
possible.</li>
</ul>
</div>
<div id="another-kind-of-problem--a-svd-kind-of-problem-"
class="section level3" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> Another kind of
problem -a SVD kind of problem-</h3>
<p>If we were to put all those multivariate observations together in one
matrix, <strong>the goal would be</strong>:</p>
<ul>
<li><p>to <em>find the best matrix</em> created <strong>with fewer
variables</strong> (lower rank) that still <strong>explains</strong> the
original data.</p></li>
<li><p>This goal is perhaps better characterized as <strong>lossy data
compression</strong>, where we want to find a smaller representation of
the original data. And one way to think about that problem is with the
<strong>singular value decomposition</strong> (SVD).</p></li>
</ul>
</div>
</div>
<div id="related-solutions-svd-and-pca." class="section level2"
number="3.9">
<h2><span class="header-section-number">3.9</span> Related solutions:
SVD and PCA.</h2>
<div id="svd-matrix-decomposition." class="section level3"
number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> SVD matrix
decomposition.</h3>
<p>If <span class="math inline">\(X\)</span> is a matrix with <em>each
variable in a column</em> and <em>each observation in a row</em> then
the <strong>SVD</strong> is a <strong><em>“matrix
decomposition”</em></strong> that represents <span
class="math inline">\(X\)</span> as a matrix product of three
matrices:</p>
<p><span class="math display">\[ X = UDV^T\]</span> where:</p>
<ul>
<li>the <strong>columns of <span
class="math inline">\(U\)</span></strong> are
<em>orthogonal/independent/uncorrelated</em> (<strong>left singular
vectors</strong> of <span class="math inline">\(X\)</span>);<br />
</li>
<li>the <strong>columns of <span
class="math inline">\(V\)</span></strong> are
<em>orthogonal/independent/uncorrelated</em> (<strong>right singular
vectors</strong> of <span class="math inline">\(X\)</span>);<br />
</li>
<li>and <strong><span class="math inline">\(D\)</span></strong> is a
<em>diagonal matrix</em> (<strong>singular values</strong> of <span
class="math inline">\(X\)</span>).</li>
</ul>
</div>
<div id="pca-as-an-application-of-svd." class="section level3"
number="3.9.2">
<h3><span class="header-section-number">3.9.2</span> PCA as an
application of SVD.</h3>
<p>As <a href="http://arxiv.org/pdf/1404.1100.pdf">Jonathon Shlens’
tutorial</a> says:</p>
<ul>
<li>PCA is <em>“a simple, non-parametric method for extracting relevant
information from<em> </em>confusing data sets”</em>.</li>
</ul>
<p>And principal components analysis (PCA) is closely related to SVD,
it’s an application of the SVD.</p>
<p>The <em>principal components</em> <strong>are equal</strong> to the
<em>right singular values</em> (the <span
class="math inline">\(V\)</span> matrix) <strong>if</strong> we
<em>first scale</em> the variables by:</p>
<ul>
<li><p><strong>substracting</strong> the column <em>mean</em> from each
column, and</p></li>
<li><p><strong>dividing</strong> each column by its <em>standard
deviation</em>.</p></li>
<li><p>This can be done with <strong>the <code>scale()</code>
function</strong> in R.</p></li>
</ul>
<p>In fact:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>mat[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>mat[<span class="dv">2</span>, ] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">7</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>mat</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>     [,<span class="dv">1</span>] [,<span class="dv">2</span>] [,<span class="dv">3</span>]</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,]    <span class="dv">1</span>    <span class="dv">2</span>    <span class="dv">3</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,]    <span class="dv">2</span>    <span class="dv">5</span>    <span class="dv">7</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="fu">scale</span>(mat)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>           [,<span class="dv">1</span>]       [,<span class="dv">2</span>]       [,<span class="dv">3</span>]</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="sc">-</span><span class="fl">0.7071068</span> <span class="sc">-</span><span class="fl">0.7071068</span> <span class="sc">-</span><span class="fl">0.7071068</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,]  <span class="fl">0.7071068</span>  <span class="fl">0.7071068</span>  <span class="fl">0.7071068</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(,<span class="st">&quot;scaled:center&quot;</span>)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="fl">1.5</span> <span class="fl">3.5</span> <span class="fl">5.0</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(,<span class="st">&quot;scaled:scale&quot;</span>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="fl">0.7071068</span> <span class="fl">2.1213203</span> <span class="fl">2.8284271</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="fu">svd</span>(<span class="fu">scale</span>(mat))</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>d</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="fl">1.732051</span> <span class="fl">0.000000</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>u</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>           [,<span class="dv">1</span>]      [,<span class="dv">2</span>]</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="sc">-</span><span class="fl">0.7071068</span> <span class="fl">0.7071068</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,]  <span class="fl">0.7071068</span> <span class="fl">0.7071068</span></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>v</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>          [,<span class="dv">1</span>]       [,<span class="dv">2</span>]</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="fl">0.5773503</span> <span class="sc">-</span><span class="fl">0.5773503</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,] <span class="fl">0.5773503</span>  <span class="fl">0.7886751</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,] <span class="fl">0.5773503</span> <span class="sc">-</span><span class="fl">0.2113249</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="fu">prcomp</span>(<span class="fu">scale</span>(mat))</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>Standard <span class="fu">deviations</span> (<span class="dv">1</span>, .., <span class="at">p=</span><span class="dv">2</span>)<span class="sc">:</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="fl">1.732051</span> <span class="fl">0.000000</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="fu">Rotation</span> (n x k) <span class="ot">=</span> (<span class="dv">3</span> x <span class="dv">2</span>)<span class="sc">:</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>           PC1        PC2</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="fl">0.5773503</span> <span class="sc">-</span><span class="fl">0.5773503</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,] <span class="fl">0.5773503</span>  <span class="fl">0.7886751</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,] <span class="fl">0.5773503</span> <span class="sc">-</span><span class="fl">0.2113249</span></span></code></pre></div>
<ul>
<li><p>The <em>principal components of the scaled matrix</em>, shown in
the Rotation component of the <code>prcomp</code> output,
<strong>ARE</strong> the <em>columns of <span
class="math inline">\(V\)</span></em>, the <em>right singular
values</em>.</p></li>
<li><p>Thus, <strong>PCA of a scaled matrix yields the <span
class="math inline">\(V\)</span> matrix</strong> (right singular
vectors) of the <strong>same scaled matrix</strong>.</p></li>
</ul>
</div>
<div id="unpacking-the-svd-the-svd-function." class="section level3"
number="3.9.3">
<h3><span class="header-section-number">3.9.3</span> Unpacking the SVD:
the <code>svd()</code> function.</h3>
<p>The SVD can be computed in R using the <code>svd()</code>
function.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(svd)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span> (x, <span class="at">nu =</span> <span class="fu">min</span>(n, p), <span class="at">nv =</span> <span class="fu">min</span>(n, p), <span class="at">LINPACK =</span> <span class="cn">FALSE</span>)  </span></code></pre></div>
<p>It <strong>returns</strong> a list with three components:</p>
<ul>
<li><code>d</code>:
<ul>
<li>a <strong>vector</strong> containing the <em>singular values</em> of
<code>x</code>, sorted decreasingly.<br />
</li>
</ul></li>
<li><code>u</code>:
<ul>
<li>a <strong>matrix</strong> whose columns contain the <em>left
singular vectors</em> of <code>x</code>.<br />
</li>
</ul></li>
<li><code>v</code>:
<ul>
<li>a <strong>matrix</strong> whose columns contain the <em>right
singular vectors</em> of <code>x</code>.</li>
</ul></li>
</ul>
<p>A fast and quick example.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>mat</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>     [,<span class="dv">1</span>] [,<span class="dv">2</span>] [,<span class="dv">3</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,]    <span class="dv">1</span>    <span class="dv">2</span>    <span class="dv">3</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,]    <span class="dv">2</span>    <span class="dv">5</span>    <span class="dv">7</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>mat_svd <span class="ot">&lt;-</span> <span class="fu">svd</span>(mat)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>mat_svd</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>d</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="fl">9.5899624</span> <span class="fl">0.1806108</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>u</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>           [,<span class="dv">1</span>]       [,<span class="dv">2</span>]</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="sc">-</span><span class="fl">0.3897782</span> <span class="sc">-</span><span class="fl">0.9209087</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,] <span class="sc">-</span><span class="fl">0.9209087</span>  <span class="fl">0.3897782</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>v</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>           [,<span class="dv">1</span>]       [,<span class="dv">2</span>]</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="sc">-</span><span class="fl">0.2327012</span> <span class="sc">-</span><span class="fl">0.7826345</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,] <span class="sc">-</span><span class="fl">0.5614308</span>  <span class="fl">0.5928424</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,] <span class="sc">-</span><span class="fl">0.7941320</span> <span class="sc">-</span><span class="fl">0.1897921</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>mat_svd<span class="sc">$</span>u <span class="sc">%*%</span> <span class="fu">diag</span>(mat_svd<span class="sc">$</span>d) <span class="sc">%*%</span> <span class="fu">t</span>(mat_svd<span class="sc">$</span>v)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>     [,<span class="dv">1</span>] [,<span class="dv">2</span>] [,<span class="dv">3</span>]</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,]    <span class="dv">1</span>    <span class="dv">2</span>    <span class="dv">3</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,]    <span class="dv">2</span>    <span class="dv">5</span>    <span class="dv">7</span></span></code></pre></div>
<p>Now, let’s <code>scale</code> our original matrix (with the pattern
in it) data and apply the <code>svd</code>:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>svd1 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrixOrdered))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(svd1)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>List of <span class="dv">3</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> d<span class="sc">:</span> num [<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>] <span class="fl">12.46</span> <span class="fl">7.78</span> <span class="fl">6.73</span> <span class="fl">6.3</span> <span class="fl">5.86</span> ...</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> u<span class="sc">:</span> num [<span class="dv">1</span><span class="sc">:</span><span class="dv">40</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>] <span class="sc">-</span><span class="fl">0.1411</span> <span class="sc">-</span><span class="fl">0.1389</span> <span class="sc">-</span><span class="fl">0.1533</span> <span class="sc">-</span><span class="fl">0.1288</span> <span class="sc">-</span><span class="fl">0.0891</span> ...</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a> <span class="sc">$</span> v<span class="sc">:</span> num [<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>] <span class="sc">-</span><span class="fl">0.0127</span> <span class="fl">0.1196</span> <span class="fl">0.0334</span> <span class="fl">0.0941</span> <span class="sc">-</span><span class="fl">0.122</span> ...</span></code></pre></div>
<p>Now we plot the <strong>first</strong> left and right singular
vectors along with the original data:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(dataMatrixOrdered)[, <span class="fu">nrow</span>(dataMatrixOrdered)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;Original Data&quot;</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>u[, <span class="dv">1</span>], <span class="dv">40</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">xlab =</span> <span class="st">&quot;First left singular vector&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Row&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>v[, <span class="dv">1</span>], <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;First right singular vector&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-38-1.png" width="1152" /></p>
<ul>
<li>We can see how the first left and right singular vectors
<strong>pick up the mean shift</strong> in both the rows and columns of
the matrix.
<ul>
<li>We don’t show this, but the other columns of <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> don’t show this pattern as clearly as
these first one do.<br />
</li>
</ul></li>
<li>The nice thing about the SVD is that <strong>it immediately picked
up on the shift</strong> in the means from the row dimension and the
column dimension, without you having to tell it anything. It was
<strong>totally unsupervised</strong>, it just picked it up
automatically.<br />
</li>
<li>We made a plot that was very similar to this before, but that was
when we knew that there was this pattern in the data set. So here the,
the SVD picked up the pattern right away and in the first left and right
singular vectors.</li>
</ul>
<p><strong>Why were the first columns of both the <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> matrices so special?</strong></p>
<ul>
<li>Well as it happens, <em>the <span class="math inline">\(D\)</span>
matrix</em> of the SVD <em>explains this phenomenon</em>. It is an
aspect of SVD called <strong>variance explained</strong>.<br />
</li>
<li>Recall that <span class="math inline">\(D\)</span> is the diagonal
matrix <em>sandwiched</em> in between <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span>^t in the SVD representation of the data
matrix.<br />
</li>
<li>The <em>diagonal entries</em> of <span
class="math inline">\(D\)</span> are <strong>like weights</strong> for
the <span class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> columns accounting for the
<em>variation in the data</em>.<br />
</li>
<li>They’re given in <em>decreasing order</em> from highest to
lowest.</li>
</ul>
</div>
<div id="svd-for-data-compression." class="section level3"
number="3.9.4">
<h3><span class="header-section-number">3.9.4</span> SVD for data
compression.</h3>
<p>If we <strong><em>believed</em></strong> that the
<strong>first</strong> <em>left</em> and <em>right singular
vectors</em>, call them <code>u1</code> and <code>v1</code>,
<strong><em>captured</em></strong> all of the variation in the data,
then <strong><em>we could</em></strong>
<strong><em>approximate</em></strong> the original data matrix
<strong><em>with</em></strong>:</p>
<p><span class="math display">\[X \approx u_1 v_1^T\]</span> Thus, we
would reduce 400 numbers in the original matrix to 40 + 10 = 50 numbers
in the compressed matrix, a <strong><em>nearly 90%
reduction</em></strong> in information.</p>
<p>Here’s what the original data and the approximation would look
like.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Approximate original data with outer product of first singular vectors.</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>approx <span class="ot">&lt;-</span> <span class="fu">with</span>(svd1, <span class="fu">outer</span>(u[, <span class="dv">1</span>], v[, <span class="dv">1</span>]))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot original data and approximated data.</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(dataMatrixOrdered)[, <span class="fu">nrow</span>(dataMatrixOrdered)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;Original Matrix&quot;</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(approx)[, <span class="fu">nrow</span>(approx)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;Approximated Matrix&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-39-1.png" width="960" /></p>
<ul>
<li>The approximation seems reasonable in this case.<br />
</li>
<li>This is not surprising given that there was only one real feature in
the original data.</li>
</ul>
</div>
</div>
<div
id="statistical-interpretation-of-components-of-the-svd-variance-explained-by-each-singular-value."
class="section level2" number="3.10">
<h2><span class="header-section-number">3.10</span> Statistical
interpretation of components of the SVD: variance explained by each
singular value.</h2>
<p>Turning back to the meaning of the <span
class="math inline">\(D\)</span> matrix produced by the SVD, we can say
then that the <em>statistical interpretation of singular values</em> is
in the form of <em>variance</em> in the data <em>explained by the
various components</em>.</p>
<p>The <em>singular values</em> produced by the <code>svd()</code>
function are in order <em>from largest to</em> <em>smallest</em> and
<strong><em>when squared</em></strong> are <em>proportional to the
amount of variance explained</em> by a given singular vector.</p>
<p>To show how this works, here’s a <strong>very simple
example</strong>.</p>
<ul>
<li>First, we’ll simulate a “dataset” that <strong>just takes two
values</strong>, 0 and 1.</li>
</ul>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>constantMatrix <span class="ot">&lt;-</span> dataMatrixOrdered <span class="sc">*</span> <span class="dv">0</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(dataMatrixOrdered)[<span class="dv">1</span>]) {</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        constantMatrix[i, ] <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">each =</span> <span class="dv">5</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(constantMatrix)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>     [,<span class="dv">1</span>] [,<span class="dv">2</span>] [,<span class="dv">3</span>] [,<span class="dv">4</span>] [,<span class="dv">5</span>] [,<span class="dv">6</span>] [,<span class="dv">7</span>] [,<span class="dv">8</span>] [,<span class="dv">9</span>] [,<span class="dv">10</span>]</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,]    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>     <span class="dv">1</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,]    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>     <span class="dv">1</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,]    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>     <span class="dv">1</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>[<span class="dv">4</span>,]    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>     <span class="dv">1</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>[<span class="dv">5</span>,]    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>     <span class="dv">1</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>[<span class="dv">6</span>,]    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">0</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>    <span class="dv">1</span>     <span class="dv">1</span></span></code></pre></div>
<ul>
<li>Then we can <strong>take the SVD</strong> of this matrix and
<strong>show the singular values</strong> as well as the
<strong>proportion of variance explained</strong> by each singular
value.</li>
</ul>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>svd2 <span class="ot">&lt;-</span> <span class="fu">svd</span>(constantMatrix)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(constantMatrix)[, <span class="fu">nrow</span>(constantMatrix)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;Original Data&quot;</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd2<span class="sc">$</span>d, <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Singular value&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd2<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="fu">sum</span>(svd2<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span>), <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Prop. of variance explained&quot;</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-41-1.png" width="1152" /></p>
<ul>
<li>As we can see from the right-most plot, <strong>100% of the
variation</strong> in this “dataset” is <strong>explained</strong> by
the <strong>first singular value</strong>.<br />
</li>
<li>In other words, <strong>all of the variation</strong> in this
dataset <strong>occurs</strong> in a <strong>single</strong>
<strong>dimension</strong>.<br />
</li>
<li>This is clear because all of the variation in the data occurs as we
go from left to right across the columns. Otherwise, the values of the
data are constant.</li>
</ul>
<p><strong>Turning back</strong> to our slightly more complex dataset
that we’d been using previously, let’s plot the <strong>singular
values</strong> (left) and the <strong>proportion of variance
explained</strong>:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>svd1 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrixOrdered))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>d, <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Singular value&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="fu">sum</span>(svd1<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span>), <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Prop. of variance explained&quot;</span>,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<ul>
<li>We can see that the <strong>first component explains</strong>
<strong><em>about 40%</em></strong> of all the variation in the
data.<br />
</li>
<li>In other words, even though there are 10 dimensions in the data,
<em>40% of the</em> <em>variation</em> in the data can be <em>explained
by a single dimension</em>.<br />
</li>
<li>That <strong>suggests</strong> that the data could be
<strong>simplified</strong> quite a bit, a phenomenon we observed in the
last section where it appeared the data could be reasonably
<strong>approximated</strong> by the <em>first left and right singular
vectors</em>.</li>
</ul>
</div>
<div
id="the-related-principal-components-analysis--pca--prcomp-function."
class="section level2" number="3.11">
<h2><span class="header-section-number">3.11</span> The related
principal components analysis -PCA-: <code>prcomp()</code>
function.</h2>
<p>As we mentioned above, the SVD has a close connection to principal
components analysis (PCA).</p>
<p>PCA can be applied to the data by calling <strong>the
<code>prcomp()</code> function</strong> in R.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(prcomp)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span> (x, ...)  </span></code></pre></div>
<ul>
<li>It <strong>returns</strong> a list with class <code>prcomp</code>
containing the following components:
<ul>
<li><code>sdev</code>:
<ul>
<li>the standard deviation of the principal components;<br />
</li>
<li>i.e., the square roots of the eigenvalues of the
covariance/correlation matrix, though the calculation is actually done
with the singular values of the data matrix.<br />
</li>
</ul></li>
<li><code>rotation</code>:
<ul>
<li>the matrix of variable loadings;<br />
</li>
<li>i.e., a matrix whose columns contain the eigenvectors.<br />
</li>
</ul></li>
<li>The <code>center</code> and <code>scale</code> used.</li>
</ul></li>
</ul>
<p>Here we show that the <strong>first right singular vector</strong>
from the <code>SVD</code> is <strong>equal</strong> to the <strong>first
principal component vector returned</strong> by <code>PCA</code>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>svd1 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrixOrdered))</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>pca1 <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(dataMatrixOrdered, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pca1<span class="sc">$</span>rotation[, <span class="dv">1</span>], svd1<span class="sc">$</span>v[, <span class="dv">1</span>], <span class="at">pch=</span><span class="dv">19</span>, </span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Principal Component 1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Right Singular Vector 1&quot;</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-44-1.png" width="576" /></p>
<blockquote>
<p>Whether you call this procedure SVD or PCA really just depends on
<strong>who you talk to</strong>:<br />
- <em>Statisticians</em> and people with that kind of background will
typically call it <code>PCA</code>,<br />
- while <em>engineers and mathematicians</em> will tend to call it
<code>SVD</code>.</p>
</blockquote>
</div>
<div id="what-if-we-add-a-second-pattern" class="section level2"
number="3.12">
<h2><span class="header-section-number">3.12</span> What if we add a
second pattern?</h2>
<p>Tracking a single patter in a matrix is relatively straightforward,
but <strong>typically</strong> there will be <strong>multiple layered
patterns</strong> in a matrix of data.</p>
<p>Here <strong>we add two patterns</strong> to a simulated dataset.</p>
<ul>
<li><p><em>One pattern</em> simple adds a <em>constant</em> to the
<em>last 5 columns</em> of data,</p></li>
<li><p>while the <em>other pattern</em> adds <em>an alternating
pattern</em> (every other column).</p></li>
</ul>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">678910</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">40</span>) {</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># flip a coin</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  coinFlip1 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fl">0.5</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>  coinFlip2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fl">0.5</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># if coin is heads add a common pattern to that row</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(coinFlip1){</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    dataMatrix[i, ] <span class="ot">&lt;-</span> dataMatrix[i, ] <span class="sc">+</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="at">each =</span> <span class="dv">5</span>)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(coinFlip2){</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    dataMatrix[i, ] <span class="ot">&lt;-</span> dataMatrix[i, ] <span class="sc">+</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dv">5</span>)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>hh <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(dataMatrix))</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>dataMatrixOrdered <span class="ot">&lt;-</span> dataMatrix[hh<span class="sc">$</span>order, ]</span></code></pre></div>
<div id="plotting-the-true-patterns-which-normally-we-ignore."
class="section level3" number="3.12.1">
<h3><span class="header-section-number">3.12.1</span> Plotting the
<em>true</em> patterns which normally we ignore.</h3>
<p>We can plot this new dataset along with the two different patterns
that we know in advanced because we put them there.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(dataMatrixOrdered)[, <span class="fu">nrow</span>(dataMatrixOrdered)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;Data&quot;</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">each =</span> <span class="dv">5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Pattern 1&quot;</span>,</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Block pattern&quot;</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Pattern 2&quot;</span>,</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Alternating pattern&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-46-1.png" width="1152" /></p>
<ul>
<li>The middle plot shows the true difference between the <em>left and
right columns</em>.<br />
</li>
<li>The rightmost plot shows the true difference between the <em>odd
numbered</em> and <em>even-numbered</em> columns.</li>
</ul>
</div>
<div id="can-svdpca-analysis-detect-these-patterns-just-from-the-data"
class="section level3" number="3.12.2">
<h3><span class="header-section-number">3.12.2</span> Can SVD/PCA
analysis detect these patterns just from the data?</h3>
<div id="v-to-pick-up-column-patterns-of-variance."
class="section level4" number="3.12.2.1">
<h4><span class="header-section-number">3.12.2.1</span> <span
class="math inline">\(V\)</span> to pick up column patterns of
variance.</h4>
<p>Since we’re <strong>interested in patterns on columns</strong> we’ll
look at the <strong>first two</strong> <strong>right singular
vectors</strong> (columns of <span class="math inline">\(V\)</span>) to
see if they show any evidence of the patterns.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>svd2 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrixOrdered))</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>svd2<span class="sc">$</span>v[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>            [,<span class="dv">1</span>]         [,<span class="dv">2</span>]</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>,] <span class="fl">0.06154540</span>  <span class="fl">0.142468636</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a> [<span class="dv">2</span>,] <span class="fl">0.26433096</span>  <span class="fl">0.504510087</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a> [<span class="dv">3</span>,] <span class="fl">0.04987554</span>  <span class="fl">0.316470664</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a> [<span class="dv">4</span>,] <span class="fl">0.27693897</span>  <span class="fl">0.524499356</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a> [<span class="dv">5</span>,] <span class="fl">0.14275820</span> <span class="sc">-</span><span class="fl">0.282921362</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a> [<span class="dv">6</span>,] <span class="fl">0.43252652</span> <span class="sc">-</span><span class="fl">0.002280468</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a> [<span class="dv">7</span>,] <span class="fl">0.37724057</span> <span class="sc">-</span><span class="fl">0.354403893</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a> [<span class="dv">8</span>,] <span class="fl">0.43280767</span>  <span class="fl">0.039226153</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a> [<span class="dv">9</span>,] <span class="fl">0.34912246</span> <span class="sc">-</span><span class="fl">0.376485206</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>[<span class="dv">10</span>,] <span class="fl">0.43379723</span> <span class="sc">-</span><span class="fl">0.031422705</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(dataMatrixOrdered)[, <span class="fu">nrow</span>(dataMatrixOrdered)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;Data&quot;</span>)</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd2<span class="sc">$</span>v[, <span class="dv">1</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;First right singular vector&quot;</span>)</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd2<span class="sc">$</span>v[, <span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Second right singular vector&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-47-1.png" width="1152" /></p>
<ul>
<li>We can see that the <em>first right singular vector</em> seems to
pick up both the alternating pattern as well as the block/step pattern
in the data.<br />
</li>
<li>The <em>second right singular vector</em> seems to pick up a similar
pattern.<br />
</li>
<li>Making a closer inspection, we have:
<ul>
<li>In the middle plot we can see that the last 5 columns have higher
entries than the first 5. This picks up, or at least <em>alludes
to</em>, the <em>first pattern</em> we added in which affected the last
5 columns of the matrix.<br />
</li>
<li>The rightmost plot, showing the second column of <span
class="math inline">\(V\)</span>, looks more random. However, we can see
that the entries alternate or bounce up and down as we move from left to
right. This <em>hints</em> at the <em>second pattern</em> we added in
which affected only even columns of selected rows.<br />
</li>
</ul></li>
<li>This example is meant to show us that <strong>it’s hard to see
patterns</strong>, <em>even</em> <em>straightforward ones</em>.</li>
</ul>
</div>
<div id="d-and-variance-explained." class="section level4"
number="3.12.2.2">
<h4><span class="header-section-number">3.12.2.2</span> <span
class="math inline">\(D\)</span> and variance explained.</h4>
<p>When we look at the variance explained, we can see that the <em>first
singular vector</em> picks up a <em>little more than 50% of the
variation</em> in the data.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>svd1 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrixOrdered))</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>d, <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Singular value&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="fu">sum</span>(svd1<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span>), <span class="at">xlab =</span> <span class="st">&quot;Column&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Percent of variance explained&quot;</span>,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-48-1.png" width="768" /></p>
<ul>
<li>On the right-hand panel we can see that the <strong>first
component</strong> explains over 50% of the total variation in the
dataset.<br />
</li>
<li>The <strong>second component</strong> only captures about 18% or so
of the variation and it kind of trails off from there.<br />
</li>
<li>That indicates that the <strong>shift or block pattern is
stronger</strong>. It represents a large amount of variation in the data
set.<br />
</li>
<li>Also <strong>the two patterns confound each other</strong> so
they’re harder to separate and see clearly. This is what often happens
with real data.</li>
</ul>
</div>
</div>
</div>
<div id="dealing-with-missing-values." class="section level2"
number="3.13">
<h2><span class="header-section-number">3.13</span> Dealing with missing
values.</h2>
<p>Missing values are a <strong>problem that plagues any data
analysis</strong> and the analysis of matrix data is no exception.</p>
<p>Most SVD and PCA routines simply <em>cannot be applied</em> if there
are missing values in the dataset.</p>
<p>In the event of missing data, there are typically a series of
<strong>questions</strong> that should be asked:</p>
<ul>
<li>Determine the reason for the missing data; what is the process that
lead to the data being missing?<br />
</li>
<li>Is the proportion of missing values so high as to invalidate any
sort of analysis?</li>
<li>Is there information in the dataset that would allow us to
predict/infer the values of the missing data?</li>
</ul>
<p>In the <strong>example</strong> below, we take our dataset and
<em>randomly insert</em> some missing data.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>dataMatrix2 <span class="ot">&lt;-</span> dataMatrixOrdered</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Randomly insert some missing data</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>dataMatrix2[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="at">size =</span> <span class="dv">40</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)] <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>svd1 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrix2))  <span class="do">## Doesn&#39;t work!</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>Error <span class="cf">in</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrix2))<span class="sc">:</span> infinite or missing values <span class="cf">in</span> <span class="st">&#39;x&#39;</span></span></code></pre></div>
<div id="the-impute-package-from-bioconductor." class="section level3"
number="3.13.1">
<h3><span class="header-section-number">3.13.1</span> The
<code>impute</code> package from Bioconductor.</h3>
<p>Since in this case we know that the <em>missing data</em> appeared
<em>completely randomly</em> in the data, it <strong>would make sense to
try to impute</strong> the values so that we can run the SVD.</p>
<p>Here, we use <strong>the <code>impute</code> package</strong> to do a
<em>k-nearest-neighbors imputation</em> of the missing data:
<code>impute.knn()</code>.</p>
<ul>
<li>The <code>impute</code> package is available from
Bioconductor.<br />
</li>
<li>The <code>impute.knn()</code> function takes missing values in a row
and imputes it by the k nearest neighbors to that row. So if
<code>k</code> for example is 5, then it will take the five rows that
are closest to the row with the missing data, and then impute the data
in that missing row with the average of the other five rows.</li>
</ul>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(impute)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>dataMatrix2 <span class="ot">&lt;-</span> <span class="fu">impute.knn</span>(dataMatrix2)<span class="sc">$</span>data</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>svd1 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrixOrdered))</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>svd2 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(dataMatrix2))</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>v[, <span class="dv">1</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">main =</span> <span class="st">&quot;Original dataset&quot;</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd2<span class="sc">$</span>v[, <span class="dv">1</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">main =</span> <span class="st">&quot;Imputed dataset&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-50-1.png" width="768" /></p>
<ul>
<li>We can see that the results are not identical but they are
<strong>pretty close</strong>, so imputation didn’t have a mayor effect
in the running of the SVD.<br />
</li>
<li>Obviously, the missing data process was pretty simple in this case
and is likely to be more complex in other situations.</li>
</ul>
</div>
</div>
<div id="face-example-producing-approximations-with-svd."
class="section level2" number="3.14">
<h2><span class="header-section-number">3.14</span> Face example:
producing approximations with SVD.</h2>
<p>In this example, we use some data that make up an image of a face
to:</p>
<ul>
<li><p>show how the SVD can be used to <strong>produce varying
approximations</strong> to this “dataset”;</p></li>
<li><p>show how SVD (and PCA) work as a <strong>data compression
technique</strong>.</p></li>
</ul>
<p>Here is the original data.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;4_data/face.rda&quot;</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(faceData)[, <span class="fu">nrow</span>(faceData)<span class="sc">:</span><span class="dv">1</span>])</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-51-1.png" width="576" /></p>
<ul>
<li>We’ll use SVD and see how the first several components contain most
of the information in the file so that <strong>storing a huge matrix
might not be necessary</strong>.</li>
</ul>
<div id="variance-explained." class="section level3" number="3.14.1">
<h3><span class="header-section-number">3.14.1</span> Variance
explained.</h3>
<p>If we <strong>take the SVD</strong> and <strong>plot the squared and
normalized singular values</strong>, we can see that the <em>data can be
explained</em> by <em>just a few singular vectors</em>, maybe 4 or
5.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>svd1 <span class="ot">&lt;-</span> <span class="fu">svd</span>(<span class="fu">scale</span>(faceData))</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svd1<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="fu">sum</span>(svd1<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span>), <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Singular vector&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Variance explained&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-52-1.png" width="576" /></p>
</div>
<div id="create-approximations." class="section level3" number="3.14.2">
<h3><span class="header-section-number">3.14.2</span> Create
approximations.</h3>
<p>Now we can start constructing approximations to the data using the
<strong>left and right</strong> <strong>singular vectors</strong>.</p>
<p>Here we create one using <strong>just the first</strong> left and
right singular vectors.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Note that %*% is matrix multiplication.  Here svd1$d[1] is a constant.</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>approx1 <span class="ot">&lt;-</span> (svd1<span class="sc">$</span>u[, <span class="dv">1</span>] <span class="sc">*</span> svd1<span class="sc">$</span>d[<span class="dv">1</span>]) <span class="sc">%*%</span> <span class="fu">t</span>(svd1<span class="sc">$</span>v[, <span class="dv">1</span>])</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Equivalently:</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>approx1_eq <span class="ot">&lt;-</span> svd1<span class="sc">$</span>u[, <span class="dv">1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(svd1<span class="sc">$</span>v[, <span class="dv">1</span>]) <span class="sc">*</span> svd1<span class="sc">$</span>d[<span class="dv">1</span>]</span></code></pre></div>
<ul>
<li>NOTE that because of the peculiarities of R’s casting, if we do the
scalar multiplication with the <code>* operator</code> first (before the
matrix multiplication with the <code>%*% operator</code>) we MUST
enclose the 2 arguments (svd1$u[,1] and svd1$d[1]) in parentheses.</li>
</ul>
<p>We can also create ones using <strong>5 and 10 singular
vectors</strong>, which presumably would be better approximations.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In these examples we need to make the diagonal matrix out of d.</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>approx5 <span class="ot">&lt;-</span> svd1<span class="sc">$</span>u[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>] <span class="sc">%*%</span> <span class="fu">diag</span>(svd1<span class="sc">$</span>d[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]) <span class="sc">%*%</span> <span class="fu">t</span>(svd1<span class="sc">$</span>v[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]) </span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>approx10 <span class="ot">&lt;-</span> svd1<span class="sc">$</span>u[, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>] <span class="sc">%*%</span> <span class="fu">diag</span>(svd1<span class="sc">$</span>d[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]) <span class="sc">%*%</span> <span class="fu">t</span>(svd1<span class="sc">$</span>v[, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]) </span></code></pre></div>
</div>
<div id="plot-approximations." class="section level3" number="3.14.3">
<h3><span class="header-section-number">3.14.3</span> Plot
approximations.</h3>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">4</span>))</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(approx1)[, <span class="fu">nrow</span>(approx1)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;1 vector&quot;</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(approx5)[, <span class="fu">nrow</span>(approx5)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;5 vectors&quot;</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(approx10)[, <span class="fu">nrow</span>(approx10)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;10 vectors&quot;</span>)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">t</span>(faceData)[, <span class="fu">nrow</span>(faceData)<span class="sc">:</span><span class="dv">1</span>], <span class="at">main =</span> <span class="st">&quot;Original data&quot;</span>)</span></code></pre></div>
<p><img src="4_ExploratoryDataAnalysis_Module3_files/figure-html/unnamed-chunk-55-1.png" width="1344" /></p>
<ul>
<li>The approximation using 1 singular vector is pretty poor.<br />
</li>
<li>But using 5 gets us pretty close to the truth.<br />
</li>
<li>Using 10 vectors doesn’t seem to add much to the features, maybe
just a few highlights.<br />
</li>
<li>So 5 singular vectors is a reasonable approximation in this
case.</li>
</ul>
</div>
</div>
<div id="notes-and-further-resources" class="section level2"
number="3.15">
<h2><span class="header-section-number">3.15</span> Notes and further
resources</h2>
<ul>
<li>For PCA/SVD, the <strong>scale/units</strong> of the data
<strong>matters</strong>.
<ul>
<li>It’s common to measure lot’s of different variables that come on
different scales.<br />
</li>
<li>And that can cause a problem, because if one variable is much larger
than another variable, just because the unit’s so different that will
tend to drive the principle components analysis over the singular
vectors.<br />
</li>
<li>That may not be particularly meaningful to us, so we’ll want to look
at the scale of the different columns or rows are roughly comparable to
each other.<br />
</li>
</ul></li>
<li>PC’s/SV’s <strong>may mix real patterns</strong>, as we saw in the
example with two overlayed patterns.<br />
</li>
<li>SVD Can be computationally intensive for very large matrices.
<ul>
<li>However, computing power is getting ever more powerful and and there
there are some highly optimized and specialized matrix libraries out
there for computing the singular value decomposition, so this can be
done on, on lots of kind of practical problems without too much planning
in advance.<br />
</li>
</ul></li>
<li><a
href="http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf">Advanced
data analysis from an elementary point of view</a><br />
</li>
<li><a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">Elements
of statistical learning</a><br />
</li>
<li><strong>Different approaches</strong> that are worth exploring (and
are related to the basic ideas behind PCA and SVD, which is that we want
to find the lower dimensional representation that explains most of the
variation in the dataset):
<ul>
<li><a href="http://en.wikipedia.org/wiki/Factor_analysis">Factor
analysis</a><br />
</li>
<li><a
href="http://en.wikipedia.org/wiki/Independent_component_analysis">Independent
components analysis</a><br />
</li>
<li><a
href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent
semantic analysis</a></li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="session-info." class="section level1" number="4">
<h1><span class="header-section-number">4</span> Session Info.</h1>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>R version <span class="dv">3</span>.<span class="fl">6.3</span> (<span class="dv">2020-02-29</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>Platform<span class="sc">:</span> x86_64<span class="sc">-</span>pc<span class="sc">-</span>linux<span class="sc">-</span><span class="fu">gnu</span> (<span class="dv">64</span><span class="sc">-</span>bit)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>Running under<span class="sc">:</span> Ubuntu <span class="dv">18</span>.<span class="fl">04.6</span> LTS</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>Matrix products<span class="sc">:</span> default</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>BLAS<span class="sc">:</span>   <span class="er">/</span>usr<span class="sc">/</span>lib<span class="sc">/</span>x86_64<span class="sc">-</span>linux<span class="sc">-</span>gnu<span class="sc">/</span>blas<span class="sc">/</span>libblas.so.<span class="dv">3</span>.<span class="fl">7.1</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>LAPACK<span class="sc">:</span> <span class="er">/</span>usr<span class="sc">/</span>lib<span class="sc">/</span>x86_64<span class="sc">-</span>linux<span class="sc">-</span>gnu<span class="sc">/</span>lapack<span class="sc">/</span>liblapack.so.<span class="dv">3</span>.<span class="fl">7.1</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>locale<span class="sc">:</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] LC_CTYPE<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>          LC_NUMERIC<span class="ot">=</span>C                 </span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a> [<span class="dv">3</span>] LC_TIME<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>           LC_COLLATE<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>       </span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a> [<span class="dv">5</span>] LC_MONETARY<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>       LC_MESSAGES<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>      </span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a> [<span class="dv">7</span>] LC_PAPER<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>          LC_NAME<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>          </span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a> [<span class="dv">9</span>] LC_ADDRESS<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>        LC_TELEPHONE<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>     </span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>[<span class="dv">11</span>] LC_MEASUREMENT<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span>    LC_IDENTIFICATION<span class="ot">=</span>es_AR.UTF<span class="dv">-8</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>attached base packages<span class="sc">:</span></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] stats     graphics  grDevices utils     datasets  methods   base     </span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>other attached packages<span class="sc">:</span></span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] impute_1.<span class="fl">60.0</span>      RColorBrewer_1<span class="fl">.1</span><span class="dv">-2</span> maps_3.<span class="fl">3.0</span>         stringr_1.<span class="fl">4.0</span>     </span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a> [<span class="dv">5</span>] lubridate_1.<span class="fl">9.2</span>    quantmod_0.<span class="fl">4.20</span>    TTR_0.<span class="fl">24.2</span>         xts_0.<span class="fl">12.1</span>        </span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a> [<span class="dv">9</span>] zoo_1<span class="fl">.8</span><span class="dv">-9</span>          readr_2.<span class="fl">0.1</span>        tidyr_1.<span class="fl">2.0</span>        dplyr_1.<span class="fl">0.8</span>       </span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>[<span class="dv">13</span>] reshape2_1.<span class="fl">4.4</span>     Hmisc_4<span class="fl">.4</span><span class="dv">-0</span>        ggplot2_3.<span class="fl">3.5</span>      Formula_1<span class="fl">.2</span><span class="dv">-3</span>     </span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>[<span class="dv">17</span>] survival_3<span class="fl">.4</span><span class="dv">-0</span>     lattice_0<span class="fl">.20</span><span class="dv">-45</span>    jpeg_0<span class="fl">.1-8.1</span>       gitignore_0.<span class="fl">1.3</span>   </span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>[<span class="dv">21</span>] rhdf5_2.<span class="fl">30.1</span>       httr_1.<span class="fl">4.2</span>         httpuv_1.<span class="fl">6.3</span>       sqldf_0<span class="fl">.4</span><span class="dv">-11</span>      </span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>[<span class="dv">25</span>] RSQLite_2.<span class="fl">2.0</span>      gsubfn_0<span class="fl">.7</span>         proto_1.<span class="fl">0.0</span>        DBI_1.<span class="fl">1.1</span>         </span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>[<span class="dv">29</span>] data.table_1.<span class="fl">14.0</span>  jsonlite_1.<span class="fl">7.2</span>     XML_3<span class="fl">.99-0.3</span>       xlsx_0.<span class="fl">6.3</span>        </span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>[<span class="dv">33</span>] plyr_1.<span class="fl">8.6</span>        </span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>loaded via a <span class="fu">namespace</span> (and not attached)<span class="sc">:</span></span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a> [<span class="dv">1</span>] nlme_3<span class="fl">.1</span><span class="dv">-162</span>        bit64_4.<span class="fl">0.5</span>         tools_3.<span class="fl">6.3</span>        </span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a> [<span class="dv">4</span>] backports_1.<span class="fl">4.1</span>     bslib_0.<span class="fl">3.0</span>         utf8_1.<span class="fl">2.2</span>         </span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a> [<span class="dv">7</span>] R6_2.<span class="fl">5.1</span>            KernSmooth_2<span class="fl">.23</span><span class="dv">-20</span>  rpart_4.<span class="fl">1.19</span>       </span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>[<span class="dv">10</span>] mgcv_1<span class="fl">.8</span><span class="dv">-41</span>         colorspace_2<span class="fl">.0</span><span class="dv">-3</span>    nnet_7<span class="fl">.3</span><span class="dv">-18</span>        </span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>[<span class="dv">13</span>] withr_2.<span class="fl">5.0</span>         tidyselect_1.<span class="fl">2.0</span>    gridExtra_2<span class="fl">.3</span>      </span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>[<span class="dv">16</span>] bit_4.<span class="fl">0.4</span>           curl_4.<span class="fl">3.2</span>          compiler_3.<span class="fl">6.3</span>     </span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>[<span class="dv">19</span>] chron_2<span class="fl">.3</span><span class="dv">-55</span>        cli_3.<span class="fl">6.0</span>           formatR_1<span class="fl">.14</span>       </span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>[<span class="dv">22</span>] htmlTable_2.<span class="fl">0.1</span>     labeling_0.<span class="fl">4.2</span>      sass_0.<span class="fl">4.0</span>         </span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>[<span class="dv">25</span>] scales_1.<span class="fl">1.1</span>        checkmate_2.<span class="fl">0.0</span>     askpass_1<span class="fl">.1</span>        </span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>[<span class="dv">28</span>] digest_0.<span class="fl">6.29</span>       foreign_0<span class="fl">.8</span><span class="dv">-76</span>      rmarkdown_2<span class="fl">.11</span>     </span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>[<span class="dv">31</span>] base64enc_0<span class="fl">.1</span><span class="dv">-3</span>     pkgconfig_2.<span class="fl">0.3</span>     htmltools_0.<span class="fl">5.2</span>    </span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>[<span class="dv">34</span>] fastmap_1.<span class="fl">1.0</span>       highr_0<span class="fl">.9</span>           htmlwidgets_1.<span class="fl">5.4</span>  </span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>[<span class="dv">37</span>] rlang_1.<span class="fl">0.6</span>         rstudioapi_0<span class="fl">.13</span>     farver_2.<span class="fl">1.0</span>       </span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a>[<span class="dv">40</span>] jquerylib_0.<span class="fl">1.4</span>     generics_0.<span class="fl">1.2</span>      acepack_1.<span class="fl">4.1</span>      </span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>[<span class="dv">43</span>] magrittr_2.<span class="fl">0.2</span>      Matrix_1<span class="fl">.5</span><span class="dv">-1</span>        Rcpp_1.<span class="fl">0.7</span>         </span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>[<span class="dv">46</span>] munsell_0.<span class="fl">5.0</span>       Rhdf5lib_1.<span class="fl">8.0</span>      fansi_1.<span class="fl">0.2</span>        </span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a>[<span class="dv">49</span>] clipr_0.<span class="fl">7.1</span>         lifecycle_1.<span class="fl">0.3</span>     stringi_1.<span class="fl">7.6</span>      </span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a>[<span class="dv">52</span>] yaml_2.<span class="fl">2.1</span>          grid_3.<span class="fl">6.3</span>          blob_1.<span class="fl">2.2</span>         </span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a>[<span class="dv">55</span>] promises_1.<span class="dv">2</span>.<span class="fl">0.1</span>    crayon_1.<span class="fl">5.0</span>        splines_3.<span class="fl">6.3</span>      </span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a>[<span class="dv">58</span>] xlsxjars_0.<span class="fl">6.1</span>      hms_1.<span class="fl">1.0</span>           knitr_1<span class="fl">.41</span>         </span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a>[<span class="dv">61</span>] pillar_1.<span class="fl">7.0</span>        tcltk_3.<span class="fl">6.3</span>         glue_1.<span class="fl">6.2</span>         </span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a>[<span class="dv">64</span>] evaluate_0<span class="fl">.19</span>       latticeExtra_0<span class="fl">.6</span><span class="dv">-29</span> vctrs_0.<span class="fl">5.2</span>        </span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a>[<span class="dv">67</span>] png_0<span class="fl">.1</span><span class="dv">-7</span>           tzdb_0.<span class="fl">3.0</span>          gtable_0.<span class="fl">3.0</span>       </span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a>[<span class="dv">70</span>] openssl_2.<span class="fl">0.5</span>       purrr_0.<span class="fl">3.4</span>         assertthat_0.<span class="fl">2.1</span>   </span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a>[<span class="dv">73</span>] xfun_0<span class="fl">.36</span>           later_1.<span class="fl">3.0</span>         viridisLite_0.<span class="fl">4.0</span>  </span>
<span id="cb56-58"><a href="#cb56-58" aria-hidden="true" tabindex="-1"></a>[<span class="dv">76</span>] tibble_3.<span class="fl">1.8</span>        rJava_0<span class="fl">.9</span><span class="dv">-13</span>        memoise_1.<span class="fl">1.0</span>      </span>
<span id="cb56-59"><a href="#cb56-59" aria-hidden="true" tabindex="-1"></a>[<span class="dv">79</span>] cluster_2.<span class="fl">1.4</span>       timechange_0.<span class="fl">2.0</span>    ellipsis_0.<span class="fl">3.2</span>     </span></code></pre></div>
</div>

<div style="width: 100%; padding: 10px; box-sizing: border-box; text-align: justify; border: 1px solid gray;">
  <p style="font-family: 'Playfair Display', serif; font-style: italic;">Copyright &copy; 2020 por <b>Christian A. Karanicolas</b>.  Todos los derechos reservados.  La elaboración de este sitio ha tenido como fuente principal de información el curso de <b>Especialización en Ciencias de Datos</b> brindado por la <b>Johns Hopkins University</b> a través de <b>Coursera</b>.</p>
</div>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Playfair+Display&display=swap');
</style>


<!--
<p style="font-family: 'Playfair Display', serif; font-style: italic;">Copyright &copy; 2020 por <b>Christian A. Karanicolas</b>.  Todos los derechos reservados.  La elaboración de este sitio ha tenido como fuente principal de información el curso de <b>Especialización en Ciencias de Datos</b> brindado por la <b>Johns Hopkins University</b> a través de <b>Coursera</b>.</p>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Playfair+Display&display=swap');
</style>
-->


<!--
<p style="font-style: italic;">Copyright &copy; 2020 por <b>Christian A. Karanicolas</b>.  Todos los derechos reservados.  La elaboración de este sitio ha tenido como fuente principal de información el curso de <b>Especialización en Ciencias de Datos</b> brindado por la <b>Johns Hopkins University</b> a través de <b>Coursera</b>.</p>
-->




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
